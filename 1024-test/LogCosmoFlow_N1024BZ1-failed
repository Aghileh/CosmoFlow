+------------------------------+
| CosmoFlow                    |
| # Ranks =  1024              |
| Global Batch =   1024        |
| # Parameters =   7041186     |
| CPE Plugin Pipeline Enabled  |
+------------------------------+
2018-03-23 10:54:32.647621: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.651289: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.655566: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.659275: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.662874: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.661622: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.662009: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.664427: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.663325: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.662378: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.662206: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.664380: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.667036: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.666410: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.669554: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.672669: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.671198: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.672550: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.673368: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.673871: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.675249: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.675245: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.674127: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.677603: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.674870: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.678556: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.680409: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.679379: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.678005: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.678142: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.678501: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.678070: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.681427: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.679505: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.681223: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.681602: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.681220: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.683094: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.683116: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.682365: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.683213: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.683845: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.683478: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.684616: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.683155: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.681896: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.684126: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.684194: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.685378: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.687115: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.685007: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.684845: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.685100: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.686140: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.683173: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.684154: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.684753: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.684150: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.687928: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.687672: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.687558: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.688237: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.686581: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.689202: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.690818: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.690250: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.690893: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.689059: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.691794: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.690790: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.688895: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.693604: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.691896: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.691681: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.694255: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.690605: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.690302: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.691563: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.691834: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.692560: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.691244: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.691995: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.691716: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.692695: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.694129: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.694336: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.693694: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.693724: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.694055: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.694330: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.697632: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.693518: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.696973: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.693613: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.692976: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.691494: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.697149: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.693891: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.692474: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.695700: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.693253: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.696386: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.696593: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.694392: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.698806: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.699368: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.695605: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.695407: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.696122: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.695870: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.695943: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.698014: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.697616: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.697220: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.695141: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.697102: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.695648: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.695986: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.699382: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.697544: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.697917: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.695148: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.697877: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.695923: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.697734: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.696936: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.698930: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.698265: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.699903: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.699193: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.700026: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.697650: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.701368: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.699440: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.698660: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.699279: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.697046: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.698252: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.701207: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.698547: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.703125: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.701885: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.701728: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.696963: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.700203: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.699197: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.700515: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.703813: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.701496: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.701520: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.702729: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.700404: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.701655: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.703379: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.701038: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.702232: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.702782: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.702516: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.705150: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.703610: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.702662: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.702847: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.702575: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.705443: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.703284: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.705951: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.701843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.701183: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.701357: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.701267: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.701661: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.701247: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.700297: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.703067: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.703162: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.705037: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.704041: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.705269: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.703339: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.703213: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.703177: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.705069: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.704584: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.704687: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.704532: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.707612: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.703563: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.702665: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.705949: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.706137: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.704191: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.703094: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.705456: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.703133: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.704221: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.705501: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.704195: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.705204: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.705038: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.706390: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.704359: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.705232: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.704651: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.704133: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.707014: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.704085: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.705423: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.706131: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.706042: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.710883: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.709580: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.706977: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.706461: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.704042: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.708575: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.707042: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.707476: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.707246: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.706966: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.710963: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.706612: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.708712: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.710670: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.708240: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.708604: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.708417: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.709024: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.708086: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.706135: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.708611: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.706827: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.710672: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.708425: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.709902: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.709200: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.711206: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.712726: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.710478: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.710792: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.710254: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.712314: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.712985: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.709436: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.709598: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.710310: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.712035: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.710723: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.712339: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.709849: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.709430: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.709051: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.715091: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.710279: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.709575: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.710337: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.709922: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.712453: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.709899: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.710982: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.710880: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.710058: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.711761: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.713909: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.710415: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.710663: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.712936: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.708696: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.711524: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.713964: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.714206: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.712521: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.712026: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.713892: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.713235: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.714292: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.715331: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.713758: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.712561: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.717441: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.714155: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.713045: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.713613: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.711944: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.712187: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.712594: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.714617: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.712495: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.713452: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.712752: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.712777: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.714679: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.715407: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.712277: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.712177: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.714295: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.716806: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.713105: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.714455: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.714533: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.712650: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.713502: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.715438: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.715576: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.712696: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.712761: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.712406: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.714703: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.714574: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.714595: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.713511: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.715063: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.715327: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.713898: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.715738: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.713025: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.715984: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.714587: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.714095: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.714687: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.717322: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.714452: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.716780: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.717406: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.717265: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.719574: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.717687: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.716111: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.715728: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.716721: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.717661: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.718608: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.716120: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.715898: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.719513: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.715424: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.717790: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.718147: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.718316: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.719846: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.719465: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.718518: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.716623: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.717472: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.717747: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.720876: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.721999: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.718672: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.717684: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.718427: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.717654: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.718023: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.716431: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.719326: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.719398: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.716554: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.716109: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.719385: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.717922: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.718070: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.720284: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.719636: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.717009: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.720272: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.717367: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.717119: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.717500: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.718861: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.721451: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.722147: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.721620: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.723727: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.720099: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.721931: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.718956: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.720509: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.719077: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.721343: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.722572: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.721731: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.721727: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.721760: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.721107: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.722576: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.722069: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.720608: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.722160: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.726013: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.720740: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.722882: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.724414: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.720027: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.719114: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.719385: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.719397: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.722019: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.721767: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.724253: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.722891: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.724056: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.719126: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.720202: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.719878: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.723570: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.720560: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.719921: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.720580: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.721217: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.723213: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.719621: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.721955: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.718840: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.720462: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.721164: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.720560: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.723396: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.721710: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.719439: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.720739: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.719668: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.723499: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.718049: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.724830: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.721818: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.722913: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.725232: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.726713: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.724186: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.725043: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.726237: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.725221: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.723482: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.724764: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.726218: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.722519: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.725601: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.723091: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.723926: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.727310: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.725445: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.726541: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.728162: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.721928: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.725982: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.724561: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.726277: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.724078: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.725004: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.723627: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.724925: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.725908: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.724798: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.725923: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.723715: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.723691: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.726241: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.724941: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.723847: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.726000: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.724272: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.728774: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.725787: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.726934: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.723736: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.727098: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.724063: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.727162: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.725598: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.724303: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.726395: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.723001: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.727386: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.725729: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.724811: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.725462: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.723547: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.727237: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.726805: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.726159: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.724451: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.728895: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.729027: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.727053: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.727448: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.728729: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.729035: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.730469: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.726841: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.728323: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.728646: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.726974: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.730557: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.728570: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.727906: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.727348: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.727715: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.730655: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.729114: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.730632: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.730151: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.726691: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.728840: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.728488: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.730357: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.728471: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.727673: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.726963: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.731463: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.728987: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.728585: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.729491: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.725873: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.727507: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.730189: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.730710: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.728540: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.730186: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.730677: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.727523: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.727739: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.729204: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.730142: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.730147: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.725958: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.732144: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.729832: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.729114: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.729844: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.729068: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.731609: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.731052: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.730437: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.731831: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.730446: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.733372: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.735237: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.733101: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.731194: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.730362: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.729698: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.730911: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.731126: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.733146: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.731140: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.732619: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.730194: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.729916: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.730851: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.735555: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.729912: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.733439: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.730105: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.731837: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.733793: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.733773: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.732371: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.732892: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.730947: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.733092: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.732892: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.731137: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.731334: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.731739: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.734401: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.735338: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.731658: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.734332: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.733842: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.730931: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.732583: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.732368: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.731179: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.734272: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.733630: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.734425: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.733059: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.732553: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.733183: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.731099: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.734503: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.734255: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.732421: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.730726: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.735520: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.732601: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.733643: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.733031: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.732686: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.732839: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.734232: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.733169: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.737275: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.735331: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.737315: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.736602: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.738600: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.739361: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.734492: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.735154: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.735940: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.738697: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.739491: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.740016: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.732779: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.734965: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.734353: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.733808: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.736991: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.736233: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.734374: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.735316: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.738907: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.735008: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.734783: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.735554: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.735654: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.737547: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.739675: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.734811: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.735097: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.738161: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.735545: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.736165: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.740495: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.736865: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.739580: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.736751: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.735341: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.737870: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.736969: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.736377: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.734436: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.735699: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.736910: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.735011: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.733624: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.736389: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.736399: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.739058: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.734899: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.737122: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.736220: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.736727: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.738439: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.738389: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.736035: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.738774: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.734105: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.735817: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.736418: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.740067: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.737364: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.734594: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.738827: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.740713: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.738106: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.735512: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.738528: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.737882: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.733223: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.740376: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.734727: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.738369: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.739402: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.739662: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.740782: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.737910: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.739081: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.744122: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.742659: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.744033: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.738471: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.740386: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.738084: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.740650: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.742557: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.742760: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.740464: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.738881: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.744243: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.742572: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.742865: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.739682: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.742153: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.744101: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.742376: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.739075: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.738981: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.741054: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.742081: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.743615: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.744621: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.737417: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.738932: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.743475: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.740840: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.742612: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.744908: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.738928: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.739425: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.741550: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.741691: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.740351: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.743251: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.743509: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.743658: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.739375: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.740428: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.741913: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.738568: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.743339: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.739826: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.741235: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.742703: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.742228: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.741765: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.743581: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.740133: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.745104: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.742856: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.741472: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.740097: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.742308: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.741573: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.741577: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.739716: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.740727: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.745211: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.742810: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.740889: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.743421: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.741082: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.742741: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.743944: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.742761: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.744642: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.747161: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.745469: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.745440: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.746918: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.744194: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.744940: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.745440: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.745106: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.745191: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.746550: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.745426: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.746266: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.747464: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.746837: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.745513: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.743366: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.746448: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.745213: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.746566: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.747102: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.746996: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.743882: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.749290: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.746462: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.743583: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.747008: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.746326: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.747800: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.746872: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.745389: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.745541: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.746098: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.745859: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.744479: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.746099: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.746588: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.745507: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.747443: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.747263: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.748847: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.747578: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.746797: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.745155: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.751415: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.747962: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.751562: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.748529: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.748516: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.749374: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.748947: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.750331: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.747575: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.749896: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.747865: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.752621: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.745675: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.748364: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.751021: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.752896: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.750273: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.748640: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.752568: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.750462: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.753461: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.752971: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.754248: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.749082: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.748416: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.749367: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.751031: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.754216: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.750865: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.752094: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.750652: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.751167: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.753411: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.752044: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.752776: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.752236: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.749374: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.753701: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.753041: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.756562: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.752218: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.752545: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.753810: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.753337: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.753669: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.752317: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.753164: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.752502: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.752459: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.753851: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.753230: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.753731: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.753342: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.756727: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.757039: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.756059: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.754105: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.759036: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.754362: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.753859: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.756447: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.754977: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.755739: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.756227: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.755380: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.755752: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.758917: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.755415: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.754417: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.754108: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.756525: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.756219: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.754694: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.756090: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.755377: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.757362: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.755560: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.755503: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.754630: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.758015: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.760992: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.760866: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.757727: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.756253: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.757618: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.756285: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.760940: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.757661: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.758035: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.758129: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.759245: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.759140: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.756680: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.758125: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.757465: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.758153: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.763046: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.758655: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.756981: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.758152: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.758842: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.758611: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.760045: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.759893: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.760634: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.763226: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.761309: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.763694: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.761318: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.761897: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.764963: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.758423: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.761322: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.760778: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.760355: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.760227: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.762279: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.760703: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.760825: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.761139: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.760746: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.762170: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.763440: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.763511: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.764559: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.764368: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.762422: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.761882: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.763917: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.761875: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.762059: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.763411: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.764693: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.767191: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.767298: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.763788: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.765891: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.762621: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.767719: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.763738: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.763004: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.766798: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.766745: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.767603: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.766340: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.766951: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.766020: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.770079: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.769174: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.769129: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.767055: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.767462: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.768693: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.769859: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.768035: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.769633: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.770314: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.772078: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.771052: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.768881: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.768070: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.770724: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.768946: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.772399: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.770273: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.771419: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.770012: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.770998: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.771517: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.769966: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.774640: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.769498: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.773156: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.770191: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.772783: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.773715: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.776244: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.773207: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.774524: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.776903: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.775186: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.777404: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.775596: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.776718: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.773858: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.776876: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.777269: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.776214: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.779901: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.776752: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.776630: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.781241: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.778030: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.779461: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.779563: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.778664: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.775833: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.780267: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.777222: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.783036: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.779817: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.784410: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.782091: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.780964: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.782232: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.782821: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.782757: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.783060: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.784553: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.784448: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.785805: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.786246: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.789278: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.784534: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.789351: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.789025: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.789462: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.790424: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.789754: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.788719: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.793195: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.790474: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.793863: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.794332: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.798144: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.796532: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.798975: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.794746: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.798179: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.800664: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.800301: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.800179: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.805623: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.809265: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.814633: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.815937: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.818929: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.819693: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.821915: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.827465: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.828739: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.827819: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.829100: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.835773: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.845961: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2018-03-23 10:54:32.881293: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
Rank 764 has waited a total of 0.846187 seconds for communication to complete so far on team 0 (calls = 0)
Rank 684 has waited a total of 1.048265 seconds for communication to complete so far on team 0 (calls = 0)
Rank 732 has waited a total of 0.933248 seconds for communication to complete so far on team 0 (calls = 0)
Rank 97 has waited a total of 0.779779 seconds for communication to complete so far on team 0 (calls = 0)
Rank 698 has waited a total of 0.970380 seconds for communication to complete so far on team 0 (calls = 0)
Rank 642 has waited a total of 1.002891 seconds for communication to complete so far on team 0 (calls = 0)
Rank 229 has waited a total of 0.608645 seconds for communication to complete so far on team 0 (calls = 0)
Rank 670 has waited a total of 1.136734 seconds for communication to complete so far on team 0 (calls = 0)
Rank 678 has waited a total of 0.711433 seconds for communication to complete so far on team 0 (calls = 0)
Rank 337 has waited a total of 1.015554 seconds for communication to complete so far on team 0 (calls = 0)
Rank 330 has waited a total of 0.719062 seconds for communication to complete so far on team 0 (calls = 0)
Rank 682 has waited a total of 1.007985 seconds for communication to complete so far on team 0 (calls = 0)
Rank 286 has waited a total of 1.191944 seconds for communication to complete so far on team 0 (calls = 0)
Rank 1014 has waited a total of 1.048209 seconds for communication to complete so far on team 0 (calls = 0)
Rank 288 has waited a total of 1.142817 seconds for communication to complete so far on team 0 (calls = 0)
Rank 157 has waited a total of 1.087835 seconds for communication to complete so far on team 0 (calls = 0)
Rank 355 has waited a total of 1.069022 seconds for communication to complete so far on team 0 (calls = 0)
Rank 839 has waited a total of 0.862650 seconds for communication to complete so far on team 0 (calls = 0)
Rank 378 has waited a total of 0.933954 seconds for communication to complete so far on team 0 (calls = 0)
Rank 774 has waited a total of 0.841310 seconds for communication to complete so far on team 0 (calls = 0)
Rank 466 has waited a total of 1.033572 seconds for communication to complete so far on team 0 (calls = 0)
Rank 467 has waited a total of 1.098456 seconds for communication to complete so far on team 0 (calls = 0)
Rank 423 has waited a total of 0.926047 seconds for communication to complete so far on team 0 (calls = 0)
Rank 357 has waited a total of 0.705240 seconds for communication to complete so far on team 0 (calls = 0)
Rank 27 has waited a total of 0.715517 seconds for communication to complete so far on team 0 (calls = 0)
Rank 6 has waited a total of 0.891151 seconds for communication to complete so far on team 0 (calls = 0)
Rank 666 has waited a total of 0.898019 seconds for communication to complete so far on team 0 (calls = 0)
Rank 778 has waited a total of 0.723806 seconds for communication to complete so far on team 0 (calls = 0)
Rank 998 has waited a total of 0.677122 seconds for communication to complete so far on team 0 (calls = 0)
Rank 272 has waited a total of 1.269099 seconds for communication to complete so far on team 0 (calls = 0)
Rank 1000 has waited a total of 0.742195 seconds for communication to complete so far on team 0 (calls = 0)
Rank 362 has waited a total of 1.209339 seconds for communication to complete so far on team 0 (calls = 0)
Rank 494 has waited a total of 0.840436 seconds for communication to complete so far on team 0 (calls = 0)
Rank 824 has waited a total of 1.082200 seconds for communication to complete so far on team 0 (calls = 0)
Rank 363 has waited a total of 1.150901 seconds for communication to complete so far on team 0 (calls = 0)
Rank 451 has waited a total of 0.715471 seconds for communication to complete so far on team 0 (calls = 0)
Rank 979 has waited a total of 1.205603 seconds for communication to complete so far on team 0 (calls = 0)
Rank 78 has waited a total of 0.928901 seconds for communication to complete so far on team 0 (calls = 0)
Rank 320 has waited a total of 0.948476 seconds for communication to complete so far on team 0 (calls = 0)
Rank 607 has waited a total of 1.003776 seconds for communication to complete so far on team 0 (calls = 0)
Rank 585 has waited a total of 1.030682 seconds for communication to complete so far on team 0 (calls = 0)
Rank 1004 has waited a total of 1.023812 seconds for communication to complete so far on team 0 (calls = 0)
Rank 674 has waited a total of 0.990093 seconds for communication to complete so far on team 0 (calls = 0)
Rank 983 has waited a total of 0.952706 seconds for communication to complete so far on team 0 (calls = 0)
Rank 521 has waited a total of 0.975233 seconds for communication to complete so far on team 0 (calls = 0)
Rank 81 has waited a total of 1.007833 seconds for communication to complete so far on team 0 (calls = 0)
Rank 368 has waited a total of 1.062198 seconds for communication to complete so far on team 0 (calls = 0)
Rank 104 has waited a total of 1.019403 seconds for communication to complete so far on team 0 (calls = 0)
Rank 676 has waited a total of 1.076572 seconds for communication to complete so far on team 0 (calls = 0)
Rank 346 has waited a total of 0.709684 seconds for communication to complete so far on team 0 (calls = 0)
Rank 302 has waited a total of 0.978535 seconds for communication to complete so far on team 0 (calls = 0)
Rank 787 has waited a total of 1.131347 seconds for communication to complete so far on team 0 (calls = 0)
Rank 1007 has waited a total of 1.009102 seconds for communication to complete so far on team 0 (calls = 0)
Rank 62 has waited a total of 0.527770 seconds for communication to complete so far on team 0 (calls = 0)
Rank 1008 has waited a total of 0.876070 seconds for communication to complete so far on team 0 (calls = 0)
Rank 128 has waited a total of 1.316537 seconds for communication to complete so far on team 0 (calls = 0)
Rank 107 has waited a total of 0.958419 seconds for communication to complete so far on team 0 (calls = 0)
Rank 481 has waited a total of 0.799955 seconds for communication to complete so far on team 0 (calls = 0)
Rank 327 has waited a total of 0.577368 seconds for communication to complete so far on team 0 (calls = 0)
Rank 680 has waited a total of 0.666613 seconds for communication to complete so far on team 0 (calls = 0)
Rank 702 has waited a total of 0.585960 seconds for communication to complete so far on team 0 (calls = 0)
Rank 614 has waited a total of 0.989253 seconds for communication to complete so far on team 0 (calls = 0)
Rank 812 has waited a total of 1.256405 seconds for communication to complete so far on team 0 (calls = 0)
Rank 328 has waited a total of 1.180004 seconds for communication to complete so far on team 0 (calls = 0)
Rank 989 has waited a total of 0.908156 seconds for communication to complete so far on team 0 (calls = 0)
Rank 418 has waited a total of 0.264976 seconds for communication to complete so far on team 0 (calls = 0)
Rank 132 has waited a total of 0.822515 seconds for communication to complete so far on team 0 (calls = 0)
Rank 506 has waited a total of 0.982737 seconds for communication to complete so far on team 0 (calls = 0)
Rank 0 has waited a total of 1.214502 seconds for communication to complete so far on team 0 (calls = 0)
Rank 221 has waited a total of 0.934311 seconds for communication to complete so far on team 0 (calls = 0)
Rank 794 has waited a total of 0.616001 seconds for communication to complete so far on team 0 (calls = 0)
Rank 552 has waited a total of 0.942136 seconds for communication to complete so far on team 0 (calls = 0)
Rank 663 has waited a total of 0.290637 seconds for communication to complete so far on team 0 (calls = 0)
Rank 245 has waited a total of 0.971336 seconds for communication to complete so far on team 0 (calls = 0)
Rank 135 has waited a total of 0.582318 seconds for communication to complete so far on team 0 (calls = 0)
Rank 510 has waited a total of 0.383435 seconds for communication to complete so far on team 0 (calls = 0)
Rank 159 has waited a total of 0.665645 seconds for communication to complete so far on team 0 (calls = 0)
Rank 314 has waited a total of 0.920541 seconds for communication to complete so far on team 0 (calls = 0)
Rank 358 has waited a total of 0.856919 seconds for communication to complete so far on team 0 (calls = 0)
Rank 1019 has waited a total of 0.996957 seconds for communication to complete so far on team 0 (calls = 0)
Rank 161 has waited a total of 1.095629 seconds for communication to complete so far on team 0 (calls = 0)
Rank 359 has waited a total of 0.647355 seconds for communication to complete so far on team 0 (calls = 0)
Rank 690 has waited a total of 0.955466 seconds for communication to complete so far on team 0 (calls = 0)
Rank 668 has waited a total of 0.718795 seconds for communication to complete so far on team 0 (calls = 0)
Rank 646 has waited a total of 0.945064 seconds for communication to complete so far on team 0 (calls = 0)
Rank 954 has waited a total of 0.945837 seconds for communication to complete so far on team 0 (calls = 0)
Rank 207 has waited a total of 0.442985 seconds for communication to complete so far on team 0 (calls = 0)
Rank 75 has waited a total of 1.001474 seconds for communication to complete so far on team 0 (calls = 0)
Rank 340 has waited a total of 1.061544 seconds for communication to complete so far on team 0 (calls = 0)
Rank 450 has waited a total of 1.032634 seconds for communication to complete so far on team 0 (calls = 0)
Rank 318 has waited a total of 0.854597 seconds for communication to complete so far on team 0 (calls = 0)
Rank 846 has waited a total of 0.915394 seconds for communication to complete so far on team 0 (calls = 0)
Rank 121 has waited a total of 0.933733 seconds for communication to complete so far on team 0 (calls = 0)
Rank 33 has waited a total of 0.849628 seconds for communication to complete so far on team 0 (calls = 0)
Rank 342 has waited a total of 0.816583 seconds for communication to complete so far on team 0 (calls = 0)
Rank 497 has waited a total of 1.016336 seconds for communication to complete so far on team 0 (calls = 0)
Rank 366 has waited a total of 0.845273 seconds for communication to complete so far on team 0 (calls = 0)
Rank 212 has waited a total of 1.185671 seconds for communication to complete so far on team 0 (calls = 0)
Rank 608 has waited a total of 1.206090 seconds for communication to complete so far on team 0 (calls = 0)
Rank 36 has waited a total of 0.891779 seconds for communication to complete so far on team 0 (calls = 0)
Rank 103 has waited a total of 0.777187 seconds for communication to complete so far on team 0 (calls = 0)
Rank 675 has waited a total of 1.168719 seconds for communication to complete so far on team 0 (calls = 0)
Rank 587 has waited a total of 0.852971 seconds for communication to complete so far on team 0 (calls = 0)
Rank 148 has waited a total of 0.864749 seconds for communication to complete so far on team 0 (calls = 0)
Rank 324 has waited a total of 0.992364 seconds for communication to complete so far on team 0 (calls = 0)
Rank 39 has waited a total of 0.952769 seconds for communication to complete so far on team 0 (calls = 0)
Rank 194 has waited a total of 0.933224 seconds for communication to complete so far on team 0 (calls = 0)
Rank 18 has waited a total of 0.430652 seconds for communication to complete so far on team 0 (calls = 0)
Rank 656 has waited a total of 0.658894 seconds for communication to complete so far on team 0 (calls = 0)
Rank 108 has waited a total of 0.908296 seconds for communication to complete so far on team 0 (calls = 0)
Rank 526 has waited a total of 1.019038 seconds for communication to complete so far on team 0 (calls = 0)
Rank 350 has waited a total of 1.011704 seconds for communication to complete so far on team 0 (calls = 0)
Rank 792 has waited a total of 0.839213 seconds for communication to complete so far on team 0 (calls = 0)
Rank 484 has waited a total of 1.029499 seconds for communication to complete so far on team 0 (calls = 0)
Rank 616 has waited a total of 1.006337 seconds for communication to complete so far on team 0 (calls = 0)
Rank 837 has waited a total of 0.930917 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 551 has waited a total of 0.576257 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 969 has waited a total of 0.893748 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 772 has waited a total of 0.901142 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 860 has waited a total of 0.742649 seconds for communication to complete so far on team 0 (calls = 0)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
Rank 948 has waited a total of 1.088007 seconds for communication to complete so far on team 0 (calls = 0)
    run_metadata_ptr)
Rank 882 has waited a total of 0.612268 seconds for communication to complete so far on team 0 (calls = 0)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
Rank 861 has waited a total of 1.055939 seconds for communication to complete so far on team 0 (calls = 0)
    feed_dict_tensor, options, run_metadata)
Rank 949 has waited a total of 1.194878 seconds for communication to complete so far on team 0 (calls = 0)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
Rank 906 has waited a total of 0.388787 seconds for communication to complete so far on team 0 (calls = 0)
    options, run_metadata)
Rank 115 has waited a total of 0.967504 seconds for communication to complete so far on team 0 (calls = 0)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
Rank 973 has waited a total of 1.200187 seconds for communication to complete so far on team 0 (calls = 0)
    raise type(e)(node_def, op, message)
Rank 93 has waited a total of 0.998189 seconds for communication to complete so far on team 0 (calls = 0)
tensorflow.python.framework.errors_impl.OutOfRangeErrorRank 160 has waited a total of 1.167676 seconds for communication to complete so far on team 0 (calls = 0)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tRank 534 has waited a total of 0.506550 seconds for communication to complete so far on team 0 (calls = 0)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensoRank 930 has waited a total of 1.182237 seconds for communication to complete so far on team 0 (calls = 0)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Rank 226 has waited a total of 1.103004 seconds for communication to complete so far on team 0 (calls = 0)
Rank 667 has waited a total of 0.446322 seconds for communication to complete so far on team 0 (calls = 0)
Rank 931 has waited a total of 0.280665 seconds for communication to complete so far on team 0 (calls = 0)
Rank 887 has waited a total of 0.745620 seconds for communication to complete so far on team 0 (calls = 0)
Rank 162 has waited a total of 0.923166 seconds for communication to complete so far on team 0 (calls = 0)
Rank 888 has waited a total of 0.634181 seconds for communication to complete so far on team 0 (calls = 0)
Rank 602 has waited a total of 0.639719 seconds for communication to complete so far on team 0 (calls = 0)
Rank 866 has waited a total of 0.897135 seconds for communication to complete so far on team 0 (calls = 0)
Rank 339 has waited a total of 1.017992 seconds for communication to complete so far on team 0 (calls = 0)
Rank 779 has waited a total of 0.919429 seconds for communication to complete so far on team 0 (calls = 0)
Rank 538 has waited a total of 0.937106 seconds for communication to complete so far on team 0 (calls = 0)
Rank 560 has waited a total of 0.995735 seconds for communication to complete so far on team 0 (calls = 0)
Rank 649 has waited a total of 1.125843 seconds for communication to complete so far on team 0 (calls = 0)
Rank 737 has waited a total of 1.041832 seconds for communication to complete so far on team 0 (calls = 0)
Rank 253 has waited a total of 0.791080 seconds for communication to complete so far on team 0 (calls = 0)
Rank 936 has waited a total of 1.046906 seconds for communication to complete so far on team 0 (calls = 0)
Rank 563 has waited a total of 0.655802 seconds for communication to complete so far on team 0 (calls = 0)
Rank 849 has waited a total of 1.111011 seconds for communication to complete so far on team 0 (calls = 0)
Rank 255 has waited a total of 0.802643 seconds for communication to complete so far on team 0 (calls = 0)
Rank 233 has waited a total of 0.429733 seconds for communication to complete so far on team 0 (calls = 0)
Rank 542 has waited a total of 1.003189 seconds for communication to complete so far on team 0 (calls = 0)
Rank 696 has waited a total of 0.948954 seconds for communication to complete so far on team 0 (calls = 0)
Rank 454 has waited a total of 1.170332 seconds for communication to complete so far on team 0 (calls = 0)
Rank 146 has waited a total of 0.965000 seconds for communication to complete so far on team 0 (calls = 0)
Rank 938 has waited a total of 0.677144 seconds for communication to complete so far on team 0 (calls = 0)
Rank 500 has waited a total of 1.071591 seconds for communication to complete so far on team 0 (calls = 0)
Rank 566 has waited a total of 0.852216 seconds for communication to complete so far on team 0 (calls = 0)
Rank 501 has waited a total of 1.044991 seconds for communication to complete so far on team 0 (calls = 0)
Rank 238 has waited a total of 0.718710 seconds for communication to complete so far on team 0 (calls = 0)
Rank 745 has waited a total of 0.984145 seconds for communication to complete so far on team 0 (calls = 0)
Rank 856 has waited a total of 1.167872 seconds for communication to complete so far on team 0 (calls = 0)
Rank 636 has waited a total of 1.116620 seconds for communication to complete so far on team 0 (calls = 0)
Rank 637 has waited a total of 1.136369 seconds for communication to complete so far on team 0 (calls = 0)
Rank 901 has waited a total of 1.098387 seconds for communication to complete so far on team 0 (calls = 0)
Rank 197 has waited a total of 0.919636 seconds for communication to complete so far on team 0 (calls = 0)
Rank 968 has waited a total of 0.719868 seconds for communication to complete so far on team 0 (calls = 0)
Rank 620 has waited a total of 0.920161 seconds for communication to complete so far on team 0 (calls = 0)
Rank 953 has waited a total of 0.946083 seconds for communication to complete so far on team 0 (calls = 0)
Rank 209 has waited a total of 0.396770 seconds for communication to complete so far on team 0 (calls = 0)
Rank 786 has waited a total of 0.680007 seconds for communication to complete so far on team 0 (calls = 0)
Rank 195 has waited a total of 1.024721 seconds for communication to complete so far on team 0 (calls = 0)
Rank 200 has waited a total of 1.019168 seconds for communication to complete so far on team 0 (calls = 0)
Rank 29 has waited a total of 0.897947 seconds for communication to complete so far on team 0 (calls = 0)
Rank 44 has waited a total of 1.189133 seconds for communication to complete so far on team 0 (calls = 0)
Rank 594 has waited a total of 1.174291 seconds for communication to complete so far on team 0 (calls = 0)
Rank 375 has waited a total of 0.393396 seconds for communication to complete so far on team 0 (calls = 0)
Rank 331 has waited a total of 0.940975 seconds for communication to complete so far on team 0 (calls = 0)
Rank 485 has waited a total of 1.087338 seconds for communication to complete so far on team 0 (calls = 0)
Rank 309 has waited a total of 0.950755 seconds for communication to complete so far on team 0 (calls = 0)
Rank 463 has waited a total of 0.969678 seconds for communication to complete so far on team 0 (calls = 0)
Rank 133 has waited a total of 0.793517 seconds for communication to complete so far on team 0 (calls = 0)
Rank 353 has waited a total of 1.067539 seconds for communication to complete so far on team 0 (calls = 0)
Rank 793 has waited a total of 0.793877 seconds for communication to complete so far on team 0 (calls = 0)
Rank 486 has waited a total of 0.956013 seconds for communication to complete so far on team 0 (calls = 0)
Rank 156 has waited a total of 1.006855 seconds for communication to complete so far on team 0 (calls = 0)
Rank 354 has waited a total of 0.682783 seconds for communication to complete so far on team 0 (calls = 0)
Rank 816 has waited a total of 0.861135 seconds for communication to complete so far on team 0 (calls = 0)
Rank 24 has waited a total of 1.190241 seconds for communication to complete so far on team 0 (calls = 0)
Rank 134 has waited a total of 0.952821 seconds for communication to complete so far on team 0 (calls = 0)
Rank 442 has waited a total of 0.940011 seconds for communication to complete so far on team 0 (calls = 0)
Rank 289 has waited a total of 1.182429 seconds for communication to complete so far on team 0 (calls = 0)
Rank 465 has waited a total of 1.070268 seconds for communication to complete so far on team 0 (calls = 0)
Rank 91 has waited a total of 0.897626 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 993 has waited a total of 0.562043 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 399 has waited a total of 1.306137 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 443 has waited a total of 0.813712 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 48 has waited a total of 0.967819 seconds for communication to complete so far on team 0 (calls = 0)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
Rank 422 has waited a total of 1.202735 seconds for communication to complete so far on team 0 (calls = 0)
    run_metadata_ptr)
Rank 400 has waited a total of 1.188644 seconds for communication to complete so far on team 0 (calls = 0)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
Rank 246 has waited a total of 1.012270 seconds for communication to complete so far on team 0 (calls = 0)
    feed_dict_tensor, options, run_metadata)
Rank 224 has waited a total of 0.805131 seconds for communication to complete so far on team 0 (calls = 0)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
Rank 994 has waited a total of 0.902756 seconds for communication to complete so far on team 0 (calls = 0)
    options, run_metadata)
Rank 796 has waited a total of 1.057075 seconds for communication to complete so far on team 0 (calls = 0)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
Rank 511 has waited a total of 1.050253 seconds for communication to complete so far on team 0 (calls = 0)
    raise type(e)(node_def, op, message)
Rank 269 has waited a total of 0.916813 seconds for communication to complete so far on team 0 (calls = 0)
tensorflow.python.framework.errors_impl.OutOfRangeErrorRank 291 has waited a total of 1.019167 seconds for communication to complete so far on team 0 (calls = 0)
Rank 577 has waited a total of 1.087260 seconds for communication to complete so far on team 0 (calls = 0)
Rank 995 has waited a total of 0.959205 seconds for communication to complete so far on team 0 (calls = 0)
Rank 336 has waited a total of 0.921449 seconds for communication to complete so far on team 0 (calls = 0)
Rank 490 has waited a total of 1.108640 seconds for communication to complete so far on team 0 (calls = 0)
Rank 600 has waited a total of 1.009261 seconds for communication to complete so far on team 0 (calls = 0)
Rank 270 has waited a total of 1.080789 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 138 has waited a total of 0.945811 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 556 has waited a total of 0.871070 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 140 has waited a total of 0.881860 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 118 has waited a total of 0.976815 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 294 has waited a total of 0.723549 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 492 has waited a total of 0.904539 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 822 has waited a total of 1.099591 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 1020 has waited a total of 1.007579 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 9 has waited a total of 0.659625 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 1022 has waited a total of 0.615587 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 802 has waited a total of 1.095528 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 385 has waited a total of 1.089863 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 407 has waited a total of 0.696792 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 341 has waited a total of 0.728277 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 671 has waited a total of 0.753797 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 891 has waited a total of 0.763540 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 55 has waited a total of 1.083215 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 430 has waited a total of 0.680756 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 980 has waited a total of 1.022219 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 518 has waited a total of 0.705788 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 364 has waited a total of 0.554264 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 650 has waited a total of 1.206081 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 409 has waited a total of 1.166793 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 145 has waited a total of 0.981062 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 959 has waited a total of 0.537900 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 14 has waited a total of 0.972376 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 102 has waited a total of 0.952260 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 322 has waited a total of 1.026528 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 894 has waited a total of 1.028217 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 477 has waited a total of 0.770629 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 367 has waited a total of 0.961032 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 301 has waited a total of 1.122015 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 411 has waited a total of 1.126497 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 60 has waited a total of 0.700272 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 236 has waited a total of 1.076073 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 830 has waited a total of 0.861455 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 61 has waited a total of 0.892524 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 479 has waited a total of 0.764355 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 919 has waited a total of 0.780174 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 699 has waited a total of 0.847418 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 326 has waited a total of 1.040747 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 502 has waited a total of 1.011023 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 480 has waited a total of 1.238245 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 634 has waited a total of 0.825273 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 371 has waited a total of 0.795676 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 525 has waited a total of 1.142326 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 767 has waited a total of 0.777152 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 569 has waited a total of 0.992476 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 306 has waited a total of 0.625565 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 922 has waited a total of 0.816218 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 988 has waited a total of 1.048610 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 592 has waited a total of 0.839934 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 373 has waited a total of 0.825069 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 285 has waited a total of 0.936716 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 131 has waited a total of 0.997846 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 1011 has waited a total of 0.894811 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 109 has waited a total of 1.002482 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 307 has waited a total of 1.066254 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 176 has waited a total of 0.824289 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 220 has waited a total of 0.937380 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 462 has waited a total of 0.995227 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 748 has waited a total of 0.680598 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 836 has waited a total of 0.497650 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 858 has waited a total of 1.000426 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 67 has waited a total of 1.001017 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 397 has waited a total of 1.086701 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 639 has waited a total of 1.003835 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 1 has waited a total of 1.016806 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 310 has waited a total of 1.048672 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 596 has waited a total of 0.989400 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 68 has waited a total of 1.172323 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 2 has waited a total of 0.606282 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 508 has waited a total of 0.503328 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 90 has waited a total of 1.246710 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 575 has waited a total of 0.737737 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 773 has waited a total of 1.086444 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 905 has waited a total of 1.011614 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 531 has waited a total of 0.901542 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 509 has waited a total of 0.850113 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 311 has waited a total of 0.794624 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 817 has waited a total of 0.965406 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 312 has waited a total of 1.097478 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 1016 has waited a total of 1.019839 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 686 has waited a total of 0.488153 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 444 has waited a total of 0.920624 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 840 has waited a total of 1.128900 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 862 has waited a total of 1.012767 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 356 has waited a total of 1.105851 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 401 has waited a total of 1.240809 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 137 has waited a total of 0.997193 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 885 has waited a total of 1.002400 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 797 has waited a total of 0.772538 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 996 has waited a total of 1.042095 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 1018 has waited a total of 1.000293 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 182 has waited a total of 0.857577 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 204 has waited a total of 0.336237 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 248 has waited a total of 1.009454 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 754 has waited a total of 0.910397 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 997 has waited a total of 1.003416 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 73 has waited a total of 0.907192 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 447 has waited a total of 0.804676 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 932 has waited a total of 0.989829 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 844 has waited a total of 0.919109 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 448 has waited a total of 1.013739 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 404 has waited a total of 1.173820 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 30 has waited a total of 0.909935 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 976 has waited a total of 0.686974 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 470 has waited a total of 1.204628 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 537 has waited a total of 0.483783 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 823 has waited a total of 1.000712 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 405 has waited a total of 1.047582 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 251 has waited a total of 1.029363 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 317 has waited a total of 0.962887 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 955 has waited a total of 1.051132 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 890 has waited a total of 0.943732 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 472 has waited a total of 0.553563 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 208 has waited a total of 0.964411 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 186 has waited a total of 1.105124 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 77 has waited a total of 0.468356 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 1023 has waited a total of 0.782233 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 429 has waited a total of 1.058901 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 165 has waited a total of 1.221185 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 847 has waited a total of 0.838461 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 99 has waited a total of 0.510292 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 1001 has waited a total of 0.753182 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 759 has waited a total of 0.834639 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 452 has waited a total of 0.662503 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 870 has waited a total of 0.903954 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 474 has waited a total of 0.943970 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 496 has waited a total of 0.922032 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 716 has waited a total of 0.957649 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 408 has waited a total of 1.091467 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 431 has waited a total of 0.709231 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 387 has waited a total of 0.340413 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 1003 has waited a total of 1.006495 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 343 has waited a total of 1.117033 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 79 has waited a total of 0.417594 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 783 has waited a total of 0.849303 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 564 has waited a total of 0.884300 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 872 has waited a total of 0.855328 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 410 has waited a total of 0.995519 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 147 has waited a total of 1.119434 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 1005 has waited a total of 0.749817 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 169 has waited a total of 0.786343 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 961 has waited a total of 0.882011 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 125 has waited a total of 1.148237 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 851 has waited a total of 1.043790 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 697 has waited a total of 0.968857 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 280 has waited a total of 1.008325 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 456 has waited a total of 0.604862 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 127 has waited a total of 0.932075 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 259 has waited a total of 0.794415 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 325 has waited a total of 1.209182 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 369 has waited a total of 0.979605 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 303 has waited a total of 0.769647 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 743 has waited a total of 1.135945 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 17 has waited a total of 0.891560 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 348 has waited a total of 1.078077 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 304 has waited a total of 0.996136 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 590 has waited a total of 0.973963 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 612 has waited a total of 0.995659 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 546 has waited a total of 1.030701 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 942 has waited a total of 0.767785 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 700 has waited a total of 0.844460 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 965 has waited a total of 1.060714 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 349 has waited a total of 1.105359 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 41 has waited a total of 0.644785 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 85 has waited a total of 1.080969 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 504 has waited a total of 0.943490 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 548 has waited a total of 0.773824 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 944 has waited a total of 0.878498 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 460 has waited a total of 0.558293 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 20 has waited a total of 1.083859 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 571 has waited a total of 0.828806 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 417 has waited a total of 1.281555 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 549 has waited a total of 0.505280 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 439 has waited a total of 0.943844 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 945 has waited a total of 0.837081 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 593 has waited a total of 0.825235 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 791 has waited a total of 0.821012 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 527 has waited a total of 1.139274 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 681 has waited a total of 1.073650 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 264 has waited a total of 0.918706 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 660 has waited a total of 1.017660 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 572 has waited a total of 0.832935 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 198 has waited a total of 0.920691 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 265 has waited a total of 0.880481 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 529 has waited a total of 1.195033 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 727 has waited a total of 1.107551 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 859 has waited a total of 1.086470 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 595 has waited a total of 0.891993 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 881 has waited a total of 0.893455 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 706 has waited a total of 1.102204 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 750 has waited a total of 0.934679 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 640 has waited a total of 0.879888 seconds for communication to complete so far on team 0 (calls = 0)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
Rank 530 has waited a total of 1.113596 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 662 has waited a total of 0.906860 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 707 has waited a total of 1.139619 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 619 has waited a total of 0.778495 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 883 has waited a total of 0.989537 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 201 has waited a total of 1.099675 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 729 has waited a total of 1.105949 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 752 has waited a total of 0.817727 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 598 has waited a total of 0.976723 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 180 has waited a total of 0.770701 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 576 has waited a total of 0.986110 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 532 has waited a total of 1.066315 seconds for communication to complete so far on team 0 (calls = 0)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
Rank 5 has waited a total of 1.069919 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 181 has waited a total of 0.943983 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 599 has waited a total of 1.126580 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 50 has waited a total of 1.114451 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 72 has waited a total of 0.331918 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 622 has waited a total of 0.905067 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 820 has waited a total of 0.967857 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 688 has waited a total of 1.084444 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 974 has waited a total of 1.001701 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 425 has waited a total of 1.095641 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 557 has waited a total of 1.161773 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 623 has waited a total of 0.912269 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 513 has waited a total of 1.149900 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 711 has waited a total of 1.009057 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 910 has waited a total of 0.638953 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 250 has waited a total of 0.930056 seconds for communication to complete so far on team 0 (calls = 0)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
Rank 426 has waited a total of 1.139066 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 712 has waited a total of 0.628891 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 52 has waited a total of 0.912969 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 756 has waited a total of 1.093474 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 427 has waited a total of 0.984154 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 581 has waited a total of 0.738241 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 757 has waited a total of 0.748365 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 10 has waited a total of 1.223070 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 539 has waited a total of 1.006709 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 715 has waited a total of 1.121145 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 187 has waited a total of 0.966519 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 935 has waited a total of 1.022482 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 232 has waited a total of 0.746045 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 210 has waited a total of 0.799923 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 672 has waited a total of 1.035621 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 760 has waited a total of 0.836804 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 804 has waited a total of 1.230502 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 628 has waited a total of 0.670747 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 914 has waited a total of 0.966099 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 958 has waited a total of 0.973040 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 739 has waited a total of 1.029452 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 937 has waited a total of 0.961345 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 211 has waited a total of 1.145355 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 695 has waited a total of 1.039017 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 519 has waited a total of 0.813773 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 58 has waited a total of 0.841983 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 828 has waited a total of 0.839431 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 124 has waited a total of 1.079843 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 652 has waited a total of 0.617085 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 718 has waited a total of 0.817634 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 850 has waited a total of 0.968957 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 631 has waited a total of 0.780271 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 895 has waited a total of 0.549289 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 763 has waited a total of 0.922505 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 257 has waited a total of 1.114496 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 191 has waited a total of 1.216576 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 543 has waited a total of 0.980694 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 917 has waited a total of 1.173918 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 192 has waited a total of 1.048206 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 412 has waited a total of 0.411359 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 918 has waited a total of 0.905034 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 962 has waited a total of 0.462203 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 258 has waited a total of 0.908358 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 874 has waited a total of 1.239491 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 193 has waited a total of 0.665149 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 831 has waited a total of 0.586176 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 611 has waited a total of 0.975815 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 897 has waited a total of 0.791173 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 875 has waited a total of 1.112506 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 721 has waited a total of 1.264888 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 963 has waited a total of 0.725355 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 898 has waited a total of 0.706044 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 40 has waited a total of 1.006469 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 964 has waited a total of 0.973382 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 172 has waited a total of 1.085376 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 260 has waited a total of 0.788985 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 722 has waited a total of 0.770635 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 150 has waited a total of 1.016336 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 833 has waited a total of 1.217153 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 261 has waited a total of 1.208210 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 217 has waited a total of 1.051964 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 63 has waited a total of 0.915072 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 591 has waited a total of 0.811178 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 613 has waited a total of 1.018847 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 855 has waited a total of 0.930306 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 834 has waited a total of 0.446286 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 570 has waited a total of 1.032828 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 724 has waited a total of 0.298743 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 262 has waited a total of 1.039196 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 835 has waited a total of 1.162651 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 21 has waited a total of 1.158822 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 879 has waited a total of 0.912285 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 219 has waited a total of 1.108558 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 617 has waited a total of 1.090715 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 683 has waited a total of 1.049008 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 641 has waited a total of 0.984437 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 335 has waited a total of 1.091943 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 908 has waited a total of 1.059632 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 755 has waited a total of 0.997628 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 205 has waited a total of 0.374488 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 185 has waited a total of 0.899548 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 738 has waited a total of 1.099391 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 455 has waited a total of 0.842038 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 853 has waited a total of 1.006920 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 568 has waited a total of 0.492175 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 659 has waited a total of 0.868770 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 535 has waited a total of 0.930358 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 947 has waited a total of 0.949528 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 751 has waited a total of 0.860837 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 740 has waited a total of 0.629359 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 95 has waited a total of 0.910434 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 1012 has waited a total of 0.746332 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 222 has waited a total of 0.839947 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 333 has waited a total of 0.987011 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 47 has waited a total of 0.637899 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 775 has waited a total of 0.732548 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 559 has waited a total of 0.952280 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 428 has waited a total of 1.158641 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 692 has waited a total of 1.168041 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 35 has waited a total of 1.080337 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 806 has waited a total of 1.269809 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 38 has waited a total of 0.898718 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 391 has waited a total of 0.471303 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 435 has waited a total of 0.536745 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 789 has waited a total of 1.066901 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 351 has waited a total of 1.047528 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 89 has waited a total of 1.159362 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 970 has waited a total of 0.610820 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 730 has waited a total of 0.308987 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 94 has waited a total of 1.139776 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 512 has waited a total of 0.957960 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 821 has waited a total of 1.016165 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 315 has waited a total of 0.898097 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 141 has waited a total of 0.933741 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 453 has waited a total of 0.760667 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 256 has waited a total of 1.029233 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 630 has waited a total of 0.997660 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 457 has waited a total of 1.113836 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Train Step: 0, Samples/Sec = 75.3827590226, Samples/Sec(inst) = 75.3827590226, Loss = 0.9241565
    trainCosmo.train()
Rank 293 has waited a total of 1.177255 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 7 has waited a total of 0.808565 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 469 has waited a total of 0.797502 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 139 has waited a total of 1.126482 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 271 has waited a total of 0.954682 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 579 has waited a total of 1.000558 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 51 has waited a total of 1.143107 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 381 has waited a total of 0.890433 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 374 has waited a total of 0.844073 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 110 has waited a total of 0.976827 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 308 has waited a total of 0.942067 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 528 has waited a total of 0.807815 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 88 has waited a total of 0.879183 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 66 has waited a total of 0.843405 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 770 has waited a total of 0.957824 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 242 has waited a total of 0.957058 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 991 has waited a total of 0.982289 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 419 has waited a total of 0.756917 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 111 has waited a total of 0.762883 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 507 has waited a total of 1.094199 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 705 has waited a total of 0.551179 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 287 has waited a total of 0.444567 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 155 has waited a total of 0.990986 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 420 has waited a total of 1.198799 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 46 has waited a total of 1.088850 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 992 has waited a total of 0.660748 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 266 has waited a total of 1.118534 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 464 has waited a total of 0.808509 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 3 has waited a total of 1.216699 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 113 has waited a total of 0.923610 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 421 has waited a total of 0.867570 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 25 has waited a total of 0.984174 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 92 has waited a total of 1.246129 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 334 has waited a total of 0.818650 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 70 has waited a total of 0.956603 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 664 has waited a total of 0.929824 seconds for communication to complete so far on team 0 (calls = 0)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
Rank 290 has waited a total of 0.872888 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 445 has waited a total of 0.974472 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 379 has waited a total of 0.859313 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 687 has waited a total of 0.744014 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 49 has waited a total of 0.871311 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 468 has waited a total of 0.936142 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 446 has waited a total of 0.819556 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 424 has waited a total of 1.094611 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 842 has waited a total of 1.042608 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 380 has waited a total of 0.682139 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 360 has waited a total of 0.815609 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 228 has waited a total of 0.751327 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 734 has waited a total of 0.844221 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 273 has waited a total of 1.058462 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 1021 has waited a total of 0.986938 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 31 has waited a total of 0.759997 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 493 has waited a total of 1.081523 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 471 has waited a total of 0.659665 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 669 has waited a total of 0.393302 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 383 has waited a total of 0.987294 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 449 has waited a total of 1.133377 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 361 has waited a total of 0.818840 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 119 has waited a total of 0.927333 seconds for communication to complete so far on team 0 (calls = 0)
    run_metadata_ptr)
Rank 295 has waited a total of 0.937649 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 274 has waited a total of 0.900511 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 296 has waited a total of 0.489290 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 54 has waited a total of 0.761183 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 780 has waited a total of 0.921402 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 473 has waited a total of 1.072051 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 319 has waited a total of 0.727822 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 11 has waited a total of 0.941671 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 143 has waited a total of 1.026501 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 605 has waited a total of 0.997293 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 297 has waited a total of 0.626697 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 56 has waited a total of 0.931248 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 1002 has waited a total of 0.155625 seconds for communication to complete so far on team 0 (calls = 0)
    run_metadata_ptr)
Rank 254 has waited a total of 0.432866 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 100 has waited a total of 1.127465 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 606 has waited a total of 1.109353 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 144 has waited a total of 1.068370 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 298 has waited a total of 0.859352 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 782 has waited a total of 0.919458 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 981 has waited a total of 1.127894 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 277 has waited a total of 1.087350 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 673 has waited a total of 1.038045 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 365 has waited a total of 0.677403 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 13 has waited a total of 0.811160 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 827 has waited a total of 0.908014 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 893 has waited a total of 0.853124 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 321 has waited a total of 0.582070 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 80 has waited a total of 0.638444 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 982 has waited a total of 0.837094 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 432 has waited a total of 0.559260 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 388 has waited a total of 0.931630 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 498 has waited a total of 0.489197 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 344 has waited a total of 0.991879 seconds for communication to complete so far on team 0 (calls = 0)
    run_metadata_ptr)
Rank 784 has waited a total of 0.847048 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 433 has waited a total of 1.093843 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 323 has waited a total of 0.797378 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 345 has waited a total of 0.556553 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 15 has waited a total of 0.885846 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 37 has waited a total of 0.884382 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 609 has waited a total of 0.778133 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 807 has waited a total of 0.958491 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 1006 has waited a total of 0.711970 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 82 has waited a total of 1.068122 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 434 has waited a total of 0.950056 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 478 has waited a total of 0.962959 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 390 has waited a total of 0.927734 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 105 has waited a total of 1.075917 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 83 has waited a total of 1.007274 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 985 has waited a total of 0.971936 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 149 has waited a total of 0.987626 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 237 has waited a total of 1.093154 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 281 has waited a total of 0.760983 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 655 has waited a total of 1.098676 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 986 has waited a total of 0.994824 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 84 has waited a total of 1.224007 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 106 has waited a total of 0.912805 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 458 has waited a total of 0.907139 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 370 has waited a total of 0.950867 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 414 has waited a total of 0.851187 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 283 has waited a total of 1.112108 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 811 has waited a total of 0.966962 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 437 has waited a total of 0.851924 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 503 has waited a total of 0.833792 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 701 has waited a total of 0.820971 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 19 has waited a total of 0.957534 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 1010 has waited a total of 0.888747 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 482 has waited a total of 1.035518 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 372 has waited a total of 0.775618 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 64 has waited a total of 0.711330 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 43 has waited a total of 0.721658 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 65 has waited a total of 1.033423 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 615 has waited a total of 0.880301 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 661 has waited a total of 0.859302 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 926 has waited a total of 0.696306 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 332 has waited a total of 1.094444 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 4 has waited a total of 0.739775 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 561 has waited a total of 1.102434 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 677 has waited a total of 0.891484 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 413 has waited a total of 0.795968 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 347 has waited a total of 0.742845 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 765 has waited a total of 1.095729 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 523 has waited a total of 0.774067 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 183 has waited a total of 1.052187 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 728 has waited a total of 1.139062 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 934 has waited a total of 0.888652 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 956 has waited a total of 0.815699 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 231 has waited a total of 1.002671 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 218 has waited a total of 1.083903 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 240 has waited a total of 1.064254 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 440 has waited a total of 0.892527 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 71 has waited a total of 0.523850 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 292 has waited a total of 1.318300 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 338 has waited a total of 0.846868 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 8 has waited a total of 0.892529 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 580 has waited a total of 1.002396 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 163 has waited a total of 0.883949 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 999 has waited a total of 0.808680 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 384 has waited a total of 1.094852 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 32 has waited a total of 1.210917 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 123 has waited a total of 0.940746 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 476 has waited a total of 1.052521 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 499 has waited a total of 0.943074 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 282 has waited a total of 1.161295 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 810 has waited a total of 0.971928 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 284 has waited a total of 1.144214 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 878 has waited a total of 1.077574 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 725 has waited a total of 0.996758 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 967 has waited a total of 0.989099 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 769 has waited a total of 0.516762 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 550 has waited a total of 0.779590 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 1013 has waited a total of 1.050737 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 573 has waited a total of 1.034736 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 398 has waited a total of 0.939126 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 1015 has waited a total of 1.142374 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 69 has waited a total of 1.270765 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 554 has waited a total of 1.006326 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 841 has waited a total of 0.715428 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 621 has waited a total of 0.733546 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 799 has waited a total of 1.048058 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 249 has waited a total of 0.898958 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 694 has waited a total of 1.144512 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 562 has waited a total of 1.050845 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 12 has waited a total of 0.994614 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 300 has waited a total of 0.765986 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 234 has waited a total of 1.044619 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 785 has waited a total of 0.986285 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 741 has waited a total of 1.142882 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 987 has waited a total of 1.020676 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 394 has waited a total of 1.093728 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 900 has waited a total of 0.870151 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 402 has waited a total of 0.987916 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 98 has waited a total of 0.861074 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 1009 has waited a total of 0.753221 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 112 has waited a total of 0.969249 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 487 has waited a total of 1.168726 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 268 has waited a total of 0.964789 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 1017 has waited a total of 0.685771 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 709 has waited a total of 0.930278 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 731 has waited a total of 1.072315 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 665 has waited a total of 0.860821 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 929 has waited a total of 0.951740 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 313 has waited a total of 0.955429 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 863 has waited a total of 1.012134 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 843 has waited a total of 1.080996 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 733 has waited a total of 0.952855 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 74 has waited a total of 1.121043 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 758 has waited a total of 0.876714 seconds for communication to complete so far on team 0 (calls = 0)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
Rank 76 has waited a total of 0.631176 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 164 has waited a total of 0.967079 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 275 has waited a total of 1.038649 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 584 has waited a total of 0.932121 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 34 has waited a total of 1.052871 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 892 has waited a total of 0.849357 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 826 has waited a total of 0.958601 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 278 has waited a total of 1.138395 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 520 has waited a total of 1.224353 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 588 has waited a total of 0.626256 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 742 has waited a total of 0.829841 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 896 has waited a total of 0.745326 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 524 has waited a total of 0.771809 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 151 has waited a total of 0.847938 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 415 has waited a total of 0.601522 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 966 has waited a total of 0.850528 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 42 has waited a total of 0.858887 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 153 has waited a total of 1.100589 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 154 has waited a total of 1.082400 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 990 has waited a total of 0.669435 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 244 has waited a total of 0.953946 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 708 has waited a total of 0.875338 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 951 has waited a total of 0.841554 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 116 has waited a total of 1.026067 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 689 has waited a total of 1.112993 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 382 has waited a total of 1.178457 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 316 has waited a total of 0.976398 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 515 has waited a total of 1.097247 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 120 has waited a total of 1.092853 seconds for communication to complete so far on team 0 (calls = 0)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
Rank 868 has waited a total of 0.776942 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 386 has waited a total of 1.012776 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 299 has waited a total of 1.107528 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 916 has waited a total of 0.687788 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 960 has waited a total of 1.061751 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 873 has waited a total of 1.018543 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 610 has waited a total of 1.160068 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 940 has waited a total of 0.489298 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 809 has waited a total of 0.996537 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 392 has waited a total of 1.059197 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 832 has waited a total of 0.542182 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 679 has waited a total of 0.848366 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 547 has waited a total of 1.157419 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 130 has waited a total of 0.572973 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 329 has waited a total of 1.086253 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 395 has waited a total of 1.200608 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 461 has waited a total of 0.768458 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 376 has waited a total of 0.682541 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 247 has waited a total of 0.794671 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 188 has waited a total of 0.985666 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 276 has waited a total of 1.004836 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 122 has waited a total of 0.789420 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 540 has waited a total of 0.797733 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 189 has waited a total of 1.054317 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 190 has waited a total of 0.790960 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 589 has waited a total of 0.815672 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 710 has waited a total of 0.948656 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 352 has waited a total of 0.986090 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 638 has waited a total of 0.706462 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 441 has waited a total of 1.072826 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 45 has waited a total of 1.023046 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 243 has waited a total of 1.031555 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 177 has waited a total of 0.930486 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 925 has waited a total of 0.420556 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 23 has waited a total of 1.142593 seconds for communication to complete so far on team 0 (calls = 0)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
Rank 223 has waited a total of 1.118257 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 685 has waited a total of 0.805834 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 114 has waited a total of 0.821740 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 644 has waited a total of 1.143960 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 578 has waited a total of 1.221012 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 117 has waited a total of 1.126562 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 601 has waited a total of 0.895232 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 624 has waited a total of 0.995511 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 625 has waited a total of 1.085558 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 911 has waited a total of 1.035709 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 647 has waited a total of 0.914376 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 714 has waited a total of 1.055189 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 516 has waited a total of 0.873740 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 781 has waited a total of 0.726307 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 803 has waited a total of 0.850512 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 693 has waited a total of 1.111469 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 475 has waited a total of 1.037544 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 651 has waited a total of 1.088260 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 915 has waited a total of 0.936767 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 101 has waited a total of 1.095557 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 629 has waited a total of 1.236084 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 57 has waited a total of 0.910439 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 389 has waited a total of 1.312446 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 279 has waited a total of 0.785794 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 984 has waited a total of 0.985614 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 720 has waited a total of 0.787447 seconds for communication to complete so far on team 0 (calls = 0)
    run_metadata_ptr)
Rank 808 has waited a total of 0.551089 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 16 has waited a total of 1.208021 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 126 has waited a total of 0.960201 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 766 has waited a total of 0.783279 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 788 has waited a total of 1.070509 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 129 has waited a total of 1.027998 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 877 has waited a total of 0.633989 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 459 has waited a total of 1.099296 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 657 has waited a total of 1.053237 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 393 has waited a total of 1.156972 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 416 has waited a total of 0.824528 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 87 has waited a total of 1.051963 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 483 has waited a total of 0.900892 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 505 has waited a total of 1.041863 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 396 has waited a total of 0.895411 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 726 has waited a total of 0.946860 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 22 has waited a total of 1.161666 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 924 has waited a total of 0.692535 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 771 has waited a total of 1.075709 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 903 has waited a total of 1.020694 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 815 has waited a total of 0.813018 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 749 has waited a total of 0.553916 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 178 has waited a total of 0.924736 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 838 has waited a total of 0.825455 seconds for communication to complete so far on team 0 (calls = 0)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
Rank 377 has waited a total of 0.652997 seconds for communication to complete so far on team 0 (calls = 0)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
Rank 267 has waited a total of 0.956671 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 158 has waited a total of 0.747584 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 643 has waited a total of 0.755552 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 819 has waited a total of 0.254356 seconds for communication to complete so far on team 0 (calls = 0)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
Rank 489 has waited a total of 0.745471 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 225 has waited a total of 1.075373 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 533 has waited a total of 0.632870 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 798 has waited a total of 1.154931 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 975 has waited a total of 0.792141 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 645 has waited a total of 1.018350 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 777 has waited a total of 1.054475 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 491 has waited a total of 0.405775 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 909 has waited a total of 0.607846 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 403 has waited a total of 0.171028 seconds for communication to complete so far on team 0 (calls = 0)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
Rank 227 has waited a total of 1.017124 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 514 has waited a total of 0.901142 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 536 has waited a total of 1.134451 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 800 has waited a total of 0.907643 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 691 has waited a total of 1.142082 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 867 has waited a total of 1.016206 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 735 has waited a total of 0.837176 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 648 has waited a total of 1.161075 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 582 has waited a total of 1.056402 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 912 has waited a total of 0.913025 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 736 has waited a total of 1.037386 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 825 has waited a total of 0.559083 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 871 has waited a total of 0.842225 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 762 has waited a total of 0.940767 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 829 has waited a total of 0.713156 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 59 has waited a total of 0.412765 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 235 has waited a total of 0.827782 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 213 has waited a total of 1.257976 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 654 has waited a total of 0.729178 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 522 has waited a total of 0.654598 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 544 has waited a total of 0.979317 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 170 has waited a total of 1.053542 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 941 has waited a total of 0.686211 seconds for communication to complete so far on team 0 (calls = 0)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
Rank 215 has waited a total of 1.158969 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 545 has waited a total of 1.140679 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 171 has waited a total of 0.364406 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 239 has waited a total of 0.980217 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 943 has waited a total of 0.842005 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 438 has waited a total of 0.448845 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 86 has waited a total of 1.017796 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 790 has waited a total of 0.810609 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 703 has waited a total of 1.260536 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 747 has waited a total of 0.827102 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 814 has waited a total of 1.172520 seconds for communication to complete so far on team 0 (calls = 0)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
Rank 202 has waited a total of 0.895455 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 558 has waited a total of 1.017942 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 761 has waited a total of 0.712606 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 488 has waited a total of 1.091123 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 776 has waited a total of 0.914651 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 801 has waited a total of 0.547355 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 305 has waited a total of 0.834859 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 658 has waited a total of 1.170354 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 880 has waited a total of 1.047089 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 946 has waited a total of 0.957231 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 704 has waited a total of 0.752793 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 902 has waited a total of 0.914563 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 199 has waited a total of 0.561493 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 904 has waited a total of 0.862686 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 618 has waited a total of 1.144366 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 574 has waited a total of 0.950608 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 927 has waited a total of 0.820138 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 795 has waited a total of 0.714510 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 597 has waited a total of 1.011394 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 553 has waited a total of 1.110027 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 179 has waited a total of 0.497379 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 971 has waited a total of 0.993762 seconds for communication to complete so far on team 0 (calls = 0)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
Rank 26 has waited a total of 0.972900 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 818 has waited a total of 0.848646 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 136 has waited a total of 0.606065 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 972 has waited a total of 0.533159 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 928 has waited a total of 0.935964 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 950 has waited a total of 1.056720 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 884 has waited a total of 1.055979 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 555 has waited a total of 0.463799 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 907 has waited a total of 0.635399 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 753 has waited a total of 1.003745 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 952 has waited a total of 0.977180 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 886 has waited a total of 0.736115 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 28 has waited a total of 0.629608 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 864 has waited a total of 0.909613 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 184 has waited a total of 1.055554 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 96 has waited a total of 1.082477 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 603 has waited a total of 1.008552 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 713 has waited a total of 0.966601 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 53 has waited a total of 1.134139 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 933 has waited a total of 1.036211 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 889 has waited a total of 0.604914 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 977 has waited a total of 0.809620 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 406 has waited a total of 0.740294 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 604 has waited a total of 1.346735 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 626 has waited a total of 0.750055 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 230 has waited a total of 0.712058 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 142 has waited a total of 0.494411 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 978 has waited a total of 0.594612 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 252 has waited a total of 0.603235 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 869 has waited a total of 0.801598 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 583 has waited a total of 1.210714 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 957 has waited a total of 0.575484 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 913 has waited a total of 0.460694 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 627 has waited a total of 1.219572 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 495 has waited a total of 0.852132 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 166 has waited a total of 0.893685 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 848 has waited a total of 0.951267 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 167 has waited a total of 1.169191 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 541 has waited a total of 0.622843 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 805 has waited a total of 0.857117 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 717 has waited a total of 1.150267 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 586 has waited a total of 0.847603 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 168 has waited a total of 0.594223 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 565 has waited a total of 0.714833 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 653 has waited a total of 1.278315 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 719 has waited a total of 0.989541 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 939 has waited a total of 1.016906 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 632 has waited a total of 1.021830 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 214 has waited a total of 1.107496 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 852 has waited a total of 0.947063 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 567 has waited a total of 0.914312 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 633 has waited a total of 0.742553 seconds for communication to complete so far on team 0 (calls = 0)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
Rank 436 has waited a total of 1.091765 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 744 has waited a total of 1.032138 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 876 has waited a total of 0.834127 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 920 has waited a total of 0.669158 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 216 has waited a total of 0.976681 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 854 has waited a total of 0.691431 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 899 has waited a total of 0.633763 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 635 has waited a total of 0.884851 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 723 has waited a total of 0.854843 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 173 has waited a total of 1.115278 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 921 has waited a total of 0.871939 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 152 has waited a total of 1.020860 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 768 has waited a total of 0.805569 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 174 has waited a total of 1.041735 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 196 has waited a total of 0.357161 seconds for communication to complete so far on team 0 (calls = 0)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
Rank 746 has waited a total of 0.740937 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 813 has waited a total of 1.002397 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 175 has waited a total of 0.749087 seconds for communication to complete so far on team 0 (calls = 0)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
Rank 923 has waited a total of 0.765806 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 263 has waited a total of 0.720884 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 857 has waited a total of 0.648685 seconds for communication to complete so far on team 0 (calls = 0)
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Rank 241 has waited a total of 0.965217 seconds for communication to complete so far on team 0 (calls = 0)
  File "CosmoNet.py", line 299, in train
Rank 203 has waited a total of 0.464772 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 865 has waited a total of 0.945915 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 206 has waited a total of 0.851363 seconds for communication to complete so far on team 0 (calls = 0)
    trainCosmo.train()
Rank 845 has waited a total of 0.946980 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Rank 517 has waited a total of 0.954435 seconds for communication to complete so far on team 0 (calls = 0)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Train Step: 1, Samples/Sec = 146.370996457, Samples/Sec(inst) = 2510.81928511, Loss = 1.6353785
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    trainCosmo.train()
    run_metadata_ptr)
    trainCosmo.train()
    trainCosmo.train()
    trainCosmo.train()
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
    trainCosmo.train()
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "CosmoNet.py", line 299, in train
  File "CosmoNet.py", line 299, in train
    trainCosmo.train()
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    trainCosmo.train()
  File "CosmoNet.py", line 299, in train
    trainCosmo.train()
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "CosmoNet.py", line 299, in train
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "CosmoNet.py", line 299, in train
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
    trainCosmo.train()
  File "CosmoNet.py", line 299, in train
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    run_metadata_ptr)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "CosmoNet.py", line 299, in train
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "CosmoNet.py", line 299, in train
    trainCosmo.train()
    trainCosmo.train()
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    trainCosmo.train()
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
srun: error: nid09800: task 186: Exited with exit code 1
srun: Terminating job step 11150338.0
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    trainCosmo.train()
    trainCosmo.train()
    trainCosmo.train()
  File "CosmoNet.py", line 299, in train
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "CosmoNet.py", line 299, in train
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
  File "CosmoNet.py", line 299, in train
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    trainCosmo.train()
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    trainCosmo.train()
  File "CosmoNet.py", line 299, in train
  File "CosmoNet.py", line 299, in train
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    trainCosmo.train()
    run_metadata_ptr)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "CosmoNet.py", line 299, in train
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
  File "CosmoNet.py", line 299, in train
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "CosmoNet.py", line 299, in train
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "CosmoNet.py", line 299, in train
  File "CosmoNet.py", line 299, in train
    trainCosmo.train()
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    trainCosmo.train()
    trainCosmo.train()
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "CosmoNet.py", line 299, in train
    trainCosmo.train()
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    trainCosmo.train()
  File "CosmoNet.py", line 299, in train
  File "CosmoNet.py", line 299, in train
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "CosmoNet.py", line 299, in train
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "CosmoNet.py", line 299, in train
    trainCosmo.train()
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "CosmoNet.py", line 299, in train
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "CosmoNet.py", line 299, in train
  File "CosmoNet.py", line 299, in train
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    trainCosmo.train()
  File "CosmoNet.py", line 299, in train
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    run_metadata_ptr)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "CosmoNet.py", line 299, in train
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    trainCosmo.train()
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    trainCosmo.train()
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    trainCosmo.train()
  File "CosmoNet.py", line 299, in train
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "CosmoNet.py", line 299, in train
  File "CosmoNet.py", line 299, in train
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "CosmoNet.py", line 299, in train
    trainCosmo.train()
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "CosmoNet.py", line 299, in train
  File "CosmoNet.py", line 299, in train
  File "CosmoNet.py", line 299, in train
  File "CosmoNet.py", line 299, in train
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
Traceback (most recent call last):
  File "CosmoNet.py", line 424, in <module>
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "CosmoNet.py", line 299, in train
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    run_metadata_ptr)
  File "CosmoNet.py", line 299, in train
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "CosmoNet.py", line 299, in train
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "CosmoNet.py", line 299, in train
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "CosmoNet.py", line 299, in train
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "CosmoNet.py", line 299, in train
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "CosmoNet.py", line 299, in train
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    run_metadata_ptr)
  File "CosmoNet.py", line 299, in train
  File "CosmoNet.py", line 299, in train
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "CosmoNet.py", line 299, in train
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "CosmoNet.py", line 299, in train
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    trainCosmo.train()
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    trainCosmo.train()
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "CosmoNet.py", line 299, in train
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "CosmoNet.py", line 299, in train
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    trainCosmo.train()
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "CosmoNet.py", line 299, in train
  File "CosmoNet.py", line 299, in train
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "CosmoNet.py", line 299, in train
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "CosmoNet.py", line 299, in train
  File "CosmoNet.py", line 299, in train
    run_metadata_ptr)
  File "CosmoNet.py", line 299, in train
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    run_metadata_ptr)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    trainCosmo.train()
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "CosmoNet.py", line 299, in train
  File "CosmoNet.py", line 299, in train
  File "CosmoNet.py", line 299, in train
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "CosmoNet.py", line 299, in train
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "CosmoNet.py", line 299, in train
  File "CosmoNet.py", line 299, in train
  File "CosmoNet.py", line 299, in train
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    trainCosmo.train()
    trainCosmo.train()
  File "CosmoNet.py", line 299, in train
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "CosmoNet.py", line 299, in train
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "CosmoNet.py", line 299, in train
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "CosmoNet.py", line 299, in train
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    trainCosmo.train()
  File "CosmoNet.py", line 299, in train
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "CosmoNet.py", line 299, in train
  File "CosmoNet.py", line 299, in train
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "CosmoNet.py", line 299, in train
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "CosmoNet.py", line 299, in train
  File "CosmoNet.py", line 299, in train
  File "CosmoNet.py", line 299, in train
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    trainCosmo.train()
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "CosmoNet.py", line 299, in train
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "CosmoNet.py", line 299, in train
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "CosmoNet.py", line 299, in train
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    trainCosmo.train()
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "CosmoNet.py", line 299, in train
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "CosmoNet.py", line 299, in train
  File "CosmoNet.py", line 299, in train
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "CosmoNet.py", line 299, in train
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "CosmoNet.py", line 299, in train
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    run_metadata_ptr)
    run_metadata_ptr)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    feed_dict_tensor, options, run_metadata)
    run_metadata_ptr)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "CosmoNet.py", line 299, in train
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    run_metadata_ptr)
    run_metadata_ptr)
    run_metadata_ptr)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    run_metadata_ptr)
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    run_metadata_ptr)
    run_metadata_ptr)
  File "CosmoNet.py", line 299, in train
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    run_metadata_ptr)
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    run_metadata_ptr)
    run_metadata_ptr)
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    run_metadata_ptr)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    run_metadata_ptr)
  File "CosmoNet.py", line 299, in train
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    run_metadata_ptr)
    run_metadata_ptr)
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    run_metadata_ptr)
  File "CosmoNet.py", line 299, in train
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    run_metadata_ptr)
    run_metadata_ptr)
  File "CosmoNet.py", line 299, in train
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    run_metadata_ptr)
    run_metadata_ptr)
    run_metadata_ptr)
    run_metadata_ptr)
    run_metadata_ptr)
    run_metadata_ptr)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    run_metadata_ptr)
    run_metadata_ptr)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    run_metadata_ptr)
    run_metadata_ptr)
    feed_dict_tensor, options, run_metadata)
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    feed_dict_tensor, options, run_metadata)
    feed_dict_tensor, options, run_metadata)
    options, run_metadata)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    options, run_metadata)
    options, run_metadata)
    raise type(e)(node_def, op, message)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    raise type(e)(node_def, op, message)
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeErrortensorflow.python.framework.errors_impl.OutOfRangeError    feed_dict_tensor, options, run_metadata)
tensorflow.python.framework.errors_impl.OutOfRangeError: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    options, run_metadata)
    run_metadata_ptr)
    run_metadata_ptr)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    raise type(e)(node_def, op, message)
    feed_dict_tensor, options, run_metadata)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    options, run_metadata)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    options, run_metadata)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    raise type(e)(node_def, op, message)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    raise type(e)(node_def, op, message)
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeErrortensorflow.python.framework.errors_impl.OutOfRangeErrortensorflow.python.framework.errors_impl.OutOfRangeError: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    feed_dict_tensor, options, run_metadata)
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
tensorflow.python.framework.errors_impl.OutOfRangeError    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    run_metadata_ptr)
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
tensorflow.python.framework.errors_impl.OutOfRangeError    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeError    feed_dict_tensor, options, run_metadata)
tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    options, run_metadata)
    feed_dict_tensor, options, run_metadata)
    feed_dict_tensor, options, run_metadata)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    raise type(e)(node_def, op, message)
    options, run_metadata)
    options, run_metadata)
tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    raise type(e)(node_def, op, message)
    raise type(e)(node_def, op, message)
    feed_dict_tensor, options, run_metadata)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensotensorflow.python.framework.errors_impl.OutOfRangeErrortensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    feed_dict_tensor, options, run_metadata)
    run_metadata_ptr)
tensorflow.python.framework.errors_impl.OutOfRangeError    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    options, run_metadata)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    raise type(e)(node_def, op, message)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensotensorflow.python.framework.errors_impl.OutOfRangeError    raise type(e)(node_def, op, message)
    feed_dict_tensor, options, run_metadata)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    options, run_metadata)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    feed_dict_tensor, options, run_metadata)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    raise type(e)(node_def, op, message)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    options, run_metadata)
    run_metadata_ptr)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensotensorflow.python.framework.errors_impl.OutOfRangeError    raise type(e)(node_def, op, message)
    feed_dict_tensor, options, run_metadata)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    options, run_metadata)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    raise type(e)(node_def, op, message)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    run_metadata_ptr)
tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    raise type(e)(node_def, op, message)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensoensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensoensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensotensorflow.python.framework.errors_impl.OutOfRangeErrorrflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    run_metadata_ptr)
tensorflow.python.framework.errors_impl.OutOfRangeError    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    feed_dict_tensor, options, run_metadata)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    feed_dict_tensor, options, run_metadata)
tensorflow.python.framework.errors_impl.OutOfRangeError    options, run_metadata)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    options, run_metadata)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    raise type(e)(node_def, op, message)
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

tensorflow.python.framework.errors_impl.OutOfRangeErrortensorflow.python.framework.errors_impl.OutOfRangeError    raise type(e)(node_def, op, message)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/ttensorflow.python.framework.errors_impl.OutOfRangeError    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    feed_dict_tensor, options, run_metadata)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    options, run_metadata)
    feed_dict_tensor, options, run_metadata)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    raise type(e)(node_def, op, message)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeError: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeErrorensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensotensorflow.python.framework.errors_impl.OutOfRangeError: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    feed_dict_tensor, options, run_metadata)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    options, run_metadata)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    feed_dict_tensor, options, run_metadata)
    feed_dict_tensor, options, run_metadata)
    raise type(e)(node_def, op, message)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    options, run_metadata)
    options, run_metadata)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensotensorflow.python.framework.errors_impl.OutOfRangeError    raise type(e)(node_def, op, message)
    raise type(e)(node_def, op, message)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/ttensorflow.python.framework.errors_impl.OutOfRangeErrortensorflow.python.framework.errors_impl.OutOfRangeError: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensoensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    run_metadata_ptr)
    feed_dict_tensor, options, run_metadata)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    feed_dict_tensor, options, run_metadata)
    options, run_metadata)
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
tensorflow.python.framework.errors_impl.OutOfRangeError    options, run_metadata)
    raise type(e)(node_def, op, message)
    run_metadata_ptr)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    raise type(e)(node_def, op, message)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    feed_dict_tensor, options, run_metadata)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

tensorflow.python.framework.errors_impl.OutOfRangeErrorensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/trflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    feed_dict_tensor, options, run_metadata)
    run_metadata_ptr)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
tensorflow.python.framework.errors_impl.OutOfRangeError    options, run_metadata)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    feed_dict_tensor, options, run_metadata)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    raise type(e)(node_def, op, message)
    options, run_metadata)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensotensorflow.python.framework.errors_impl.OutOfRangeError    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    feed_dict_tensor, options, run_metadata)
    options, run_metadata)
tensorflow.python.framework.errors_impl.OutOfRangeError    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    options, run_metadata)
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
tensorflow.python.framework.errors_impl.OutOfRangeError    feed_dict_tensor, options, run_metadata)
tensorflow.python.framework.errors_impl.OutOfRangeError    raise type(e)(node_def, op, message)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
tensorflow.python.framework.errors_impl.OutOfRangeErrorensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    options, run_metadata)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/trflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensoensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensoensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    run_metadata_ptr)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    run_metadata_ptr)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    feed_dict_tensor, options, run_metadata)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    feed_dict_tensor, options, run_metadata)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    options, run_metadata)
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
tensorflow.python.framework.errors_impl.OutOfRangeError    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeError    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
tensorflow.python.framework.errors_impl.OutOfRangeError: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    feed_dict_tensor, options, run_metadata)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

tensorflow.python.framework.errors_impl.OutOfRangeError    options, run_metadata)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensoensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    raise type(e)(node_def, op, message)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/trflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

tensorflow.python.framework.errors_impl.OutOfRangeError: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    run_metadata_ptr)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    feed_dict_tensor, options, run_metadata)
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    feed_dict_tensor, options, run_metadata)
    feed_dict_tensor, options, run_metadata)
    options, run_metadata)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    options, run_metadata)
    options, run_metadata)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    raise type(e)(node_def, op, message)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensotensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    raise type(e)(node_def, op, message)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeErrorensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensotensorflow.python.framework.errors_impl.OutOfRangeError: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    raise type(e)(node_def, op, message)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    options, run_metadata)
    feed_dict_tensor, options, run_metadata)
tensorflow.python.framework.errors_impl.OutOfRangeError    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/ttensorflow.python.framework.errors_impl.OutOfRangeError    raise type(e)(node_def, op, message)
    options, run_metadata)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/ttensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    raise type(e)(node_def, op, message)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensotensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    options, run_metadata)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    raise type(e)(node_def, op, message)
    options, run_metadata)
tensorflow.python.framework.errors_impl.OutOfRangeError    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/ttensorflow.python.framework.errors_impl.OutOfRangeError: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    raise type(e)(node_def, op, message)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensotensorflow.python.framework.errors_impl.OutOfRangeErrorrflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/trflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "CosmoNet.py", line 299, in train
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/trflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    run_metadata_ptr)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensoensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensoensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensoensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/trflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    run_metadata_ptr)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    feed_dict_tensor, options, run_metadata)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    options, run_metadata)
tensorflow.python.framework.errors_impl.OutOfRangeError    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    raise type(e)(node_def, op, message)
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    options, run_metadata)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/ttensorflow.python.framework.errors_impl.OutOfRangeError: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    feed_dict_tensor, options, run_metadata)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    options, run_metadata)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    run_metadata_ptr)
    raise type(e)(node_def, op, message)
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/ttensorflow.python.framework.errors_impl.OutOfRangeErrortensorflow.python.framework.errors_impl.OutOfRangeError    feed_dict_tensor, options, run_metadata)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    options, run_metadata)
    run_metadata_ptr)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/trflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    raise type(e)(node_def, op, message)
    feed_dict_tensor, options, run_metadata)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    options, run_metadata)
    run_metadata_ptr)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensoensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    raise type(e)(node_def, op, message)
    feed_dict_tensor, options, run_metadata)
tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    run_metadata_ptr)
    run_metadata_ptr)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
tensorflow.python.framework.errors_impl.OutOfRangeError    feed_dict_tensor, options, run_metadata)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensoensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    options, run_metadata)
    options, run_metadata)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    raise type(e)(node_def, op, message)
    raise type(e)(node_def, op, message)
    run_metadata_ptr)
    trainCosmo.train()
tensorflow.python.framework.errors_impl.OutOfRangeErrortensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "CosmoNet.py", line 299, in train
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    feed_dict_tensor, options, run_metadata)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensoensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    options, run_metadata)
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    run_metadata_ptr)
    run_metadata_ptr)
    raise type(e)(node_def, op, message)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    feed_dict_tensor, options, run_metadata)
    feed_dict_tensor, options, run_metadata)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    options, run_metadata)
    options, run_metadata)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
tensorflow.python.framework.errors_impl.OutOfRangeError    raise type(e)(node_def, op, message)
    raise type(e)(node_def, op, message)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/ttensorflow.python.framework.errors_impl.OutOfRangeErrortensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    run_metadata_ptr)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensoensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    feed_dict_tensor, options, run_metadata)
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    options, run_metadata)
    feed_dict_tensor, options, run_metadata)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    run_metadata_ptr)
    raise type(e)(node_def, op, message)
    options, run_metadata)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    feed_dict_tensor, options, run_metadata)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    raise type(e)(node_def, op, message)
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensotensorflow.python.framework.errors_impl.OutOfRangeErrortensorflow.python.framework.errors_impl.OutOfRangeError    options, run_metadata)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensoensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

tensorflow.python.framework.errors_impl.OutOfRangeError    run_metadata_ptr)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    run_metadata_ptr)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
srun: error: nid09697: task 88: Exited with exit code 1
srun: error: nid10519: task 419: Exited with exit code 1
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

srun: error: nid10682: task 574: Exited with exit code 1
srun: error: nid10635: task 531: Exited with exit code 1
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
srun: error: nid12467: task 819: Exited with exit code 1
srun: error: nid12534: task 886: Exited with exit code 1
    run_metadata_ptr)
srun: error: nid12667: task 1019: Exited with exit code 1
srun: error: nid12668: task 1020: Exited with exit code 1
    feed_dict_tensor, options, run_metadata)
srun: error: nid12669: task 1021: Exited with exit code 1
srun: error: nid09610: task 10: Exited with exit code 1
    options, run_metadata)
srun: error: nid09731: task 121: Exited with exit code 1
srun: error: nid09850: task 232: Exited with exit code 1
srun: error: nid10435: task 343: Exited with exit code 1
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
srun: error: nid10602: task 498: Exited with exit code 1
srun: error: nid12345: task 697: Exited with exit code 1
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
srun: error: nid12522: task 874: Exited with exit code 1
srun: error: nid12589: task 941: Exited with exit code 1
srun: error: nid12502: task 854: Exited with exit code 1
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
srun: error: nid12657: task 1009: Exited with exit code 1
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
srun: error: nid12526: task 878: Exited with exit code 1
srun: error: nid09741: task 131: Exited with exit code 1
srun: error: nid09768: task 154: Exited with exit code 1
    feed_dict_tensor, options, run_metadata)
srun: error: nid09698: task 89: Exited with exit code 1
srun: error: nid09892: task 266: Exited with exit code 1
    options, run_metadata)
srun: error: nid10591: task 487: Exited with exit code 1
srun: error: nid12357: task 709: Exited with exit code 1
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
srun: error: nid12578: task 930: Exited with exit code 1
srun: error: nid12535: task 887: Exited with exit code 1
    raise type(e)(node_def, op, message)
srun: error: nid09637: task 32: Exited with exit code 1
srun: error: nid09849: task 231: Exited with exit code 1
srun: error: nid09709: task 100: Exited with exit code 1
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
srun: error: nid10505: task 409: Exited with exit code 1
srun: error: nid10554: task 454: Exited with exit code 1
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
srun: error: nid10485: task 389: Exited with exit code 1
srun: error: nid10722: task 610: Exited with exit code 1
srun: error: nid12567: task 919: Exited with exit code 1
    run_metadata_ptr)
srun: error: nid12612: task 964: Exited with exit code 1
srun: error: nid12613: task 965: Exited with exit code 1
tensorflow.python.framework.errors_impl.OutOfRangeErrorsrun: error: nid09648: task 43: Exited with exit code 1
srun: error: nid09719: task 110: Exited with exit code 1
srun: error: nid09743: task 133: Exited with exit code 1
    options, run_metadata)
srun: error: nid09862: task 244: Exited with exit code 1
    raise type(e)(node_def, op, message)
srun: error: nid10495: task 399: Exited with exit code 1
srun: error: nid12379: task 731: Exited with exit code 1
srun: error: nid12380: task 732: Exited with exit code 1
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
srun: error: nid12601: task 953: Exited with exit code 1
srun: error: nid12626: task 978: Exited with exit code 1
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
srun: error: nid09829: task 211: Exited with exit code 1
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tsrun: error: nid10462: task 366: Exited with exit code 1
srun: error: nid10463: task 367: Exited with exit code 1
srun: error: nid10652: task 544: Exited with exit code 1
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
srun: error: nid12435: task 787: Exited with exit code 1
srun: error: nid12481: task 833: Exited with exit code 1
tensorflow.python.framework.errors_impl.OutOfRangeErrorsrun: error: nid09718: task 109: Exited with exit code 1
srun: error: nid09769: task 155: Exited with exit code 1
    feed_dict_tensor, options, run_metadata)
srun: error: nid09840: task 222: Exited with exit code 1
srun: error: nid10377: task 289: Exited with exit code 1
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
srun: error: nid12401: task 753: Exited with exit code 1
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensosrun: error: nid12491: task 843: Exited with exit code 1
srun: error: nid09903: task 277: Exited with exit code 1
srun: error: nid10624: task 520: Exited with exit code 1
    raise type(e)(node_def, op, message)
srun: error: nid10507: task 411: Exited with exit code 1
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

srun: error: nid12413: task 765: Exited with exit code 1
Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tsrun: error: nid09625: task 21: Exited with exit code 1
srun: error: nid09602: task 2: Exited with exit code 1
srun: error: nid09841: task 223: Exited with exit code 1
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
srun: error: nid09794: task 180: Exited with exit code 1
srun: error: nid12423: task 775: Exited with exit code 1
srun: error: nid12381: task 733: Exited with exit code 1
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

srun: error: nid09640: task 35: Exited with exit code 1
srun: error: nid09830: task 212: Exited with exit code 1
srun: error: nid10675: task 567: Exited with exit code 1
tensorflow.python.framework.errors_impl.OutOfRangeErrorsrun: error: nid10376: task 288: Exited with exit code 1
srun: error: nid10567: task 467: Exited with exit code 1
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
srun: error: nid09904: task 278: Exited with exit code 1
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensosrun: error: nid12391: task 743: Exited with exit code 1
srun: error: nid12313: task 665: Exited with exit code 1
    options, run_metadata)
srun: error: nid12369: task 721: Exited with exit code 1
srun: error: nid10615: task 511: Exited with exit code 1
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
srun: error: nid10745: task 633: Exited with exit code 1
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/trflow/python/framework/ops.py", line 1625, in __init__
srun: error: nid10594: task 490: Exited with exit code 1
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    run_metadata_ptr)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensosrun: error: nid09673: task 68: Exited with exit code 1
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

tensorflow.python.framework.errors_impl.OutOfRangeError    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    run_metadata_ptr)
srun: error: nid09771: task 157: Exited with exit code 1
    run_metadata_ptr)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    feed_dict_tensor, options, run_metadata)
    feed_dict_tensor, options, run_metadata)
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    run_metadata_ptr)
tensorflow.python.framework.errors_impl.OutOfRangeError    options, run_metadata)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    feed_dict_tensor, options, run_metadata)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    raise type(e)(node_def, op, message)
srun: error: nid10464: task 368: Exited with exit code 1
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

tensorflow.python.framework.errors_impl.OutOfRangeErrortensorflow.python.framework.errors_impl.OutOfRangeError    options, run_metadata)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    run_metadata_ptr)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensosrun: error: nid12404: task 756: Exited with exit code 1
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

tensorflow.python.framework.errors_impl.OutOfRangeError    feed_dict_tensor, options, run_metadata)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    run_metadata_ptr)
    run_metadata_ptr)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    feed_dict_tensor, options, run_metadata)
    feed_dict_tensor, options, run_metadata)
srun: error: nid12571: task 923: Exited with exit code 1
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    run_metadata_ptr)
tensorflow.python.framework.errors_impl.OutOfRangeError    options, run_metadata)
srun: error: nid12292: task 644: Exited with exit code 1
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    feed_dict_tensor, options, run_metadata)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    raise type(e)(node_def, op, message)
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

tensorflow.python.framework.errors_impl.OutOfRangeErrortensorflow.python.framework.errors_impl.OutOfRangeError    options, run_metadata)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
srun: error: nid10568: task 468: Exited with exit code 1
    run_metadata_ptr)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensoensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]
srun: error: nid10437: task 345: Exited with exit code 1

tensorflow.python.framework.errors_impl.OutOfRangeError    feed_dict_tensor, options, run_metadata)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensosrun: error: nid10497: task 401: Exited with exit code 1
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    run_metadata_ptr)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    feed_dict_tensor, options, run_metadata)
    run_metadata_ptr)
tensorflow.python.framework.errors_impl.OutOfRangeError    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    options, run_metadata)
    feed_dict_tensor, options, run_metadata)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    raise type(e)(node_def, op, message)
    options, run_metadata)
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    run_metadata_ptr)
tensorflow.python.framework.errors_impl.OutOfRangeError: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensotensorflow.python.framework.errors_impl.OutOfRangeError    feed_dict_tensor, options, run_metadata)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    options, run_metadata)
    run_metadata_ptr)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    raise type(e)(node_def, op, message)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    run_metadata_ptr)
    run_metadata_ptr)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensosrun: error: nid12670: task 1022: Exited with exit code 1
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    feed_dict_tensor, options, run_metadata)
    feed_dict_tensor, options, run_metadata)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

srun: error: nid09770: task 156: Exited with exit code 1
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
srun: error: nid09722: task 113: Exited with exit code 1
tensorflow.python.framework.errors_impl.OutOfRangeError    options, run_metadata)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    feed_dict_tensor, options, run_metadata)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    raise type(e)(node_def, op, message)
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

tensorflow.python.framework.errors_impl.OutOfRangeErrorsrun: error: nid09782: task 168: Exited with exit code 1
tensorflow.python.framework.errors_impl.OutOfRangeError    options, run_metadata)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensosrun: error: nid10676: task 568: Exited with exit code 1
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

srun: error: nid10687: task 579: Exited with exit code 1
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

tensorflow.python.framework.errors_impl.OutOfRangeError    run_metadata_ptr)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    run_metadata_ptr)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensosrun: error: nid09733: task 123: Exited with exit code 1
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

srun: error: nid12615: task 967: Exited with exit code 1
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
srun: error: nid09809: task 191: Exited with exit code 1
    run_metadata_ptr)
    feed_dict_tensor, options, run_metadata)
    options, run_metadata)
srun: error: nid09627: task 23: Exited with exit code 1
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
srun: error: nid12349: task 701: Exited with exit code 1
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    feed_dict_tensor, options, run_metadata)
    options, run_metadata)
    run_metadata_ptr)
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
srun: error: nid09864: task 246: Exited with exit code 1
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
tensorflow.python.framework.errors_impl.OutOfRangeError    options, run_metadata)
    raise type(e)(node_def, op, message)
    feed_dict_tensor, options, run_metadata)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    raise type(e)(node_def, op, message)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    options, run_metadata)
srun: error: nid10546: task 446: Exited with exit code 1
srun: error: nid09853: task 235: Exited with exit code 1
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

tensorflow.python.framework.errors_impl.OutOfRangeErrorensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/trflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    raise type(e)(node_def, op, message)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensotensorflow.python.framework.errors_impl.OutOfRangeErrorsrun: error: nid12647: task 999: Exited with exit code 1
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    run_metadata_ptr)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
srun: error: nid09603: task 3: Exited with exit code 1
srun: error: nid09734: task 124: Exited with exit code 1
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensosrun: error: nid09878: task 256: Exited with exit code 1
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    run_metadata_ptr)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    feed_dict_tensor, options, run_metadata)
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeError    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
tensorflow.python.framework.errors_impl.OutOfRangeError: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    options, run_metadata)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access
srun: error: nid10616: task 512: Exited with exit code 1

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

tensorflow.python.framework.errors_impl.OutOfRangeError    raise type(e)(node_def, op, message)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/ttensorflow.python.framework.errors_impl.OutOfRangeError    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    run_metadata_ptr)
srun: error: nid12527: task 879: Exited with exit code 1
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    run_metadata_ptr)
srun: error: nid09854: task 236: Exited with exit code 1
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

srun: error: nid12427: task 779: Exited with exit code 1
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    feed_dict_tensor, options, run_metadata)
    run_metadata_ptr)
srun: error: nid10644: task 536: Exited with exit code 1
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    options, run_metadata)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    raise type(e)(node_def, op, message)
    options, run_metadata)
srun: error: nid09760: task 146: Exited with exit code 1
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeErrorsrun: error: nid09884: task 258: Exited with exit code 1
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    options, run_metadata)
tensorflow.python.framework.errors_impl.OutOfRangeError: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensotensorflow.python.framework.errors_impl.OutOfRangeError    raise type(e)(node_def, op, message)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/ttensorflow.python.framework.errors_impl.OutOfRangeErrorrflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tsrun: error: nid09895: task 269: Exited with exit code 1
    run_metadata_ptr)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    run_metadata_ptr)
srun: error: nid12316: task 668: Exited with exit code 1
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
srun: error: nid12648: task 1000: Exited with exit code 1
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    feed_dict_tensor, options, run_metadata)
srun: error: nid12472: task 824: Exited with exit code 1
    run_metadata_ptr)
    run_metadata_ptr)
srun: error: nid12494: task 846: Exited with exit code 1
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
srun: error: nid10724: task 612: Exited with exit code 1
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    options, run_metadata)
srun: error: nid09879: task 257: Exited with exit code 1
    feed_dict_tensor, options, run_metadata)
    feed_dict_tensor, options, run_metadata)
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
srun: error: nid10487: task 391: Exited with exit code 1
tensorflow.python.framework.errors_impl.OutOfRangeError    raise type(e)(node_def, op, message)
    options, run_metadata)
    options, run_metadata)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/ttensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
srun: error: nid09750: task 136: Exited with exit code 1
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
srun: error: nid09663: task 58: Exited with exit code 1
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    raise type(e)(node_def, op, message)
    raise type(e)(node_def, op, message)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensotensorflow.python.framework.errors_impl.OutOfRangeErrorsrun: error: nid10583: task 479: Exited with exit code 1
tensorflow.python.framework.errors_impl.OutOfRangeErrorrflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensoensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    run_metadata_ptr)
  File "CosmoNet.py", line 299, in train
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    run_metadata_ptr)
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    feed_dict_tensor, options, run_metadata)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    options, run_metadata)
tensorflow.python.framework.errors_impl.OutOfRangeError    options, run_metadata)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    raise type(e)(node_def, op, message)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    raise type(e)(node_def, op, message)
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeErrorrflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

tensorflow.python.framework.errors_impl.OutOfRangeErrortensorflow.python.framework.errors_impl.OutOfRangeError: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensoensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
srun: error: nid10665: task 557: Exited with exit code 1
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    run_metadata_ptr)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    run_metadata_ptr)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
srun: error: nid12627: task 979: Exited with exit code 1
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    run_metadata_ptr)
srun: error: nid09690: task 81: Exited with exit code 1
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    feed_dict_tensor, options, run_metadata)
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
tensorflow.python.framework.errors_impl.OutOfRangeError    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    options, run_metadata)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/ttensorflow.python.framework.errors_impl.OutOfRangeErrorsrun: error: nid10617: task 513: Exited with exit code 1
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensosrun: error: nid12593: task 945: Exited with exit code 1
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/ttensorflow.python.framework.errors_impl.OutOfRangeError    raise type(e)(node_def, op, message)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/ttensorflow.python.framework.errors_impl.OutOfRangeErrorrflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    run_metadata_ptr)
    run_metadata_ptr)
srun: error: nid10369: task 281: Exited with exit code 1
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
srun: error: nid09628: task 24: Exited with exit code 1
    feed_dict_tensor, options, run_metadata)
    feed_dict_tensor, options, run_metadata)
    run_metadata_ptr)
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    options, run_metadata)
    options, run_metadata)
    feed_dict_tensor, options, run_metadata)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    raise type(e)(node_def, op, message)
    raise type(e)(node_def, op, message)
    options, run_metadata)
    options, run_metadata)
tensorflow.python.framework.errors_impl.OutOfRangeErrortensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    raise type(e)(node_def, op, message)
    raise type(e)(node_def, op, message)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensoensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensotensorflow.python.framework.errors_impl.OutOfRangeErrortensorflow.python.framework.errors_impl.OutOfRangeErrorrflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensoensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    run_metadata_ptr)
    run_metadata_ptr)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    feed_dict_tensor, options, run_metadata)
srun: error: nid12470: task 822: Exited with exit code 1
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    options, run_metadata)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    raise type(e)(node_def, op, message)
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    options, run_metadata)
tensorflow.python.framework.errors_impl.OutOfRangeErrortensorflow.python.framework.errors_impl.OutOfRangeError    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    raise type(e)(node_def, op, message)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensoensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeErrorrflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

tensorflow.python.framework.errors_impl.OutOfRangeError: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "CosmoNet.py", line 299, in train
    run_metadata_ptr)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    feed_dict_tensor, options, run_metadata)
    run_metadata_ptr)
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    options, run_metadata)
    feed_dict_tensor, options, run_metadata)
    run_metadata_ptr)
    feed_dict_tensor, options, run_metadata)
srun: error: nid12438: task 790: Exited with exit code 1
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    raise type(e)(node_def, op, message)
    options, run_metadata)
    feed_dict_tensor, options, run_metadata)
    options, run_metadata)
tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    raise type(e)(node_def, op, message)
    options, run_metadata)
    raise type(e)(node_def, op, message)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensotensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
tensorflow.python.framework.errors_impl.OutOfRangeErrorrflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    raise type(e)(node_def, op, message)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensosrun: error: nid10726: task 614: Exited with exit code 1
tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    run_metadata_ptr)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    run_metadata_ptr)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    feed_dict_tensor, options, run_metadata)
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
tensorflow.python.framework.errors_impl.OutOfRangeError    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    options, run_metadata)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeError    raise type(e)(node_def, op, message)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

tensorflow.python.framework.errors_impl.OutOfRangeError: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/ttensorflow.python.framework.errors_impl.OutOfRangeError: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tsrun: error: nid09652: task 47: Exited with exit code 1
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]
srun: error: nid12338: task 690: Exited with exit code 1

rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    run_metadata_ptr)
srun: error: nid10655: task 547: Exited with exit code 1
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    feed_dict_tensor, options, run_metadata)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    options, run_metadata)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    raise type(e)(node_def, op, message)
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeErrortensorflow.python.framework.errors_impl.OutOfRangeError: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensoensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    run_metadata_ptr)
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    feed_dict_tensor, options, run_metadata)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    options, run_metadata)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    raise type(e)(node_def, op, message)
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeErrorsrun: error: nid10578: task 474: Terminated
tensorflow.python.framework.errors_impl.OutOfRangeError: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensoensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    run_metadata_ptr)
srun: error: nid10374: task 286: Terminated
  File "CosmoNet.py", line 299, in train
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    run_metadata_ptr)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    feed_dict_tensor, options, run_metadata)
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
tensorflow.python.framework.errors_impl.OutOfRangeError    options, run_metadata)
srun: error: nid10661: task 553: Terminated
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
srun: error: nid12489: task 841: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    raise type(e)(node_def, op, message)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

tensorflow.python.framework.errors_impl.OutOfRangeError: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
srun: error: nid12301: task 653: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    run_metadata_ptr)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    run_metadata_ptr)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    feed_dict_tensor, options, run_metadata)
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
tensorflow.python.framework.errors_impl.OutOfRangeError    options, run_metadata)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    raise type(e)(node_def, op, message)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

tensorflow.python.framework.errors_impl.OutOfRangeError: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    run_metadata_ptr)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    feed_dict_tensor, options, run_metadata)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    options, run_metadata)
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    raise type(e)(node_def, op, message)
    feed_dict_tensor, options, run_metadata)
tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    options, run_metadata)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeError    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    run_metadata_ptr)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

srun: error: nid09707: task 98: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    feed_dict_tensor, options, run_metadata)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    options, run_metadata)
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
tensorflow.python.framework.errors_impl.OutOfRangeErrorsrun: error: nid12322: task 674: Terminated
    raise type(e)(node_def, op, message)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/ttensorflow.python.framework.errors_impl.OutOfRangeErrorensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensosrun: error: nid09614: task 14: Exited with exit code 1
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/trflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    run_metadata_ptr)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
srun: error: nid09816: task 198: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    feed_dict_tensor, options, run_metadata)
    options, run_metadata)
srun: error: nid09636: task 31: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    options, run_metadata)
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
tensorflow.python.framework.errors_impl.OutOfRangeErrorsrun: error: nid12628: task 980: Exited with exit code 1
    raise type(e)(node_def, op, message)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/ttensorflow.python.framework.errors_impl.OutOfRangeErrorensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/trflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
srun: error: nid12422: task 774: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    run_metadata_ptr)
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    feed_dict_tensor, options, run_metadata)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
srun: error: nid12334: task 686: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    options, run_metadata)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    raise type(e)(node_def, op, message)
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeErrortensorflow.python.framework.errors_impl.OutOfRangeError: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
srun: error: nid10482: task 386: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tsrun: error: nid10411: task 319: Terminated
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensoensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    run_metadata_ptr)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    feed_dict_tensor, options, run_metadata)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    options, run_metadata)
srun: error: nid10390: task 298: Terminated
    raise type(e)(node_def, op, message)
srun: error: nid10590: task 486: Terminated
srun: error: nid12622: task 974: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
tensorflow.python.framework.errors_impl.OutOfRangeError    raise type(e)(node_def, op, message)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/ttensorflow.python.framework.errors_impl.OutOfRangeErrorensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/trflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
srun: error: nid10428: task 336: Exited with exit code 1
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
srun: error: nid12434: task 786: Terminated
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    run_metadata_ptr)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    feed_dict_tensor, options, run_metadata)
srun: error: nid09832: task 214: Exited with exit code 1
    options, run_metadata)
srun: error: nid10494: task 398: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    options, run_metadata)
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
tensorflow.python.framework.errors_impl.OutOfRangeError    raise type(e)(node_def, op, message)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/ttensorflow.python.framework.errors_impl.OutOfRangeErrorensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tsrun: error: nid09761: task 147: Exited with exit code 1
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensosrun: error: nid10694: task 586: Terminated
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    run_metadata_ptr)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    feed_dict_tensor, options, run_metadata)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    options, run_metadata)
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
tensorflow.python.framework.errors_impl.OutOfRangeError    raise type(e)(node_def, op, message)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/ttensorflow.python.framework.errors_impl.OutOfRangeErrorensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
srun: error: nid12555: task 907: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/trflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

srun: error: nid10417: task 325: Exited with exit code 1
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
srun: error: nid10731: task 619: Terminated
    run_metadata_ptr)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    feed_dict_tensor, options, run_metadata)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    options, run_metadata)
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
tensorflow.python.framework.errors_impl.OutOfRangeError    raise type(e)(node_def, op, message)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/ttensorflow.python.framework.errors_impl.OutOfRangeErrorensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/trflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    run_metadata_ptr)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
srun: error: nid10710: task 598: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    feed_dict_tensor, options, run_metadata)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    options, run_metadata)
    raise type(e)(node_def, op, message)
srun: error: nid10531: task 431: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
srun: error: nid10623: task 519: Terminated
tensorflow.python.framework.errors_impl.OutOfRangeError    raise type(e)(node_def, op, message)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/ttensorflow.python.framework.errors_impl.OutOfRangeErrorsrun: error: nid10629: task 525: Exited with exit code 1
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/trflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    feed_dict_tensor, options, run_metadata)
srun: error: nid12367: task 719: Terminated
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
srun: error: nid09757: task 143: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    options, run_metadata)
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    raise type(e)(node_def, op, message)
    feed_dict_tensor, options, run_metadata)
tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    options, run_metadata)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeErrorsrun: error: nid12655: task 1007: Terminated
    run_metadata_ptr)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    feed_dict_tensor, options, run_metadata)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    feed_dict_tensor, options, run_metadata)
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
tensorflow.python.framework.errors_impl.OutOfRangeError    options, run_metadata)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    raise type(e)(node_def, op, message)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

srun: error: nid09629: task 25: Exited with exit code 1
tensorflow.python.framework.errors_impl.OutOfRangeError: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    run_metadata_ptr)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    feed_dict_tensor, options, run_metadata)
srun: error: nid12634: task 986: Terminated
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
srun: error: nid10423: task 331: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    options, run_metadata)
srun: error: nid09861: task 243: Terminated
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    raise type(e)(node_def, op, message)
    options, run_metadata)
srun: error: nid09778: task 164: Terminated
tensorflow.python.framework.errors_impl.OutOfRangeErrorsrun: error: nid09685: task 76: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    raise type(e)(node_def, op, message)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensotensorflow.python.framework.errors_impl.OutOfRangeErrorsrun: error: nid10460: task 364: Terminated
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "CosmoNet.py", line 299, in train
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
srun: error: nid09890: task 264: Terminated
    run_metadata_ptr)
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    feed_dict_tensor, options, run_metadata)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
srun: error: nid12583: task 935: Exited with exit code 1
    options, run_metadata)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    raise type(e)(node_def, op, message)
srun: error: nid10743: task 631: Terminated
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeErrortensorflow.python.framework.errors_impl.OutOfRangeError: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensoensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    run_metadata_ptr)
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    feed_dict_tensor, options, run_metadata)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
srun: error: nid10564: task 464: Terminated
    options, run_metadata)
    options, run_metadata)
srun: error: nid12300: task 652: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    raise type(e)(node_def, op, message)
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeErrortensorflow.python.framework.errors_impl.OutOfRangeError: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
srun: error: nid12400: task 752: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensoensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

srun: error: nid09902: task 276: Terminated
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
srun: error: nid12405: task 757: Exited with exit code 1
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    run_metadata_ptr)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    feed_dict_tensor, options, run_metadata)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    options, run_metadata)
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
tensorflow.python.framework.errors_impl.OutOfRangeError    raise type(e)(node_def, op, message)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tsrun: error: nid12395: task 747: Exited with exit code 1
tensorflow.python.framework.errors_impl.OutOfRangeErrorensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
srun: error: nid09790: task 176: Terminated
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/trflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

srun: error: nid10380: task 292: Exited with exit code 1
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    run_metadata_ptr)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

srun: error: nid12500: task 852: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    feed_dict_tensor, options, run_metadata)
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    options, run_metadata)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    raise type(e)(node_def, op, message)
    options, run_metadata)
tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    raise type(e)(node_def, op, message)
srun: error: nid10737: task 625: Exited with exit code 1
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensosrun: error: nid12479: task 831: Terminated
tensorflow.python.framework.errors_impl.OutOfRangeErrorrflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    run_metadata_ptr)
srun: error: nid09609: task 9: Terminated
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    feed_dict_tensor, options, run_metadata)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    options, run_metadata)
srun: error: nid10472: task 376: Terminated
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
tensorflow.python.framework.errors_impl.OutOfRangeError    raise type(e)(node_def, op, message)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/ttensorflow.python.framework.errors_impl.OutOfRangeErrorensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tsrun: error: nid12600: task 952: Terminated
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    run_metadata_ptr)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    feed_dict_tensor, options, run_metadata)
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
tensorflow.python.framework.errors_impl.OutOfRangeError    options, run_metadata)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tsrun: error: nid10601: task 497: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    raise type(e)(node_def, op, message)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

srun: error: nid10672: task 564: Terminated
tensorflow.python.framework.errors_impl.OutOfRangeError: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
srun: error: nid12579: task 931: Terminated
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    feed_dict_tensor, options, run_metadata)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensosrun: error: nid12312: task 664: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
tensorflow.python.framework.errors_impl.OutOfRangeError    run_metadata_ptr)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    feed_dict_tensor, options, run_metadata)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    options, run_metadata)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    raise type(e)(node_def, op, message)
    run_metadata_ptr)
srun: error: nid09822: task 204: Exited with exit code 1
tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
srun: error: nid09827: task 209: Terminated
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    feed_dict_tensor, options, run_metadata)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    options, run_metadata)
srun: error: nid12412: task 764: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
srun: error: nid09802: task 188: Terminated
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
tensorflow.python.framework.errors_impl.OutOfRangeError    run_metadata_ptr)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    feed_dict_tensor, options, run_metadata)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    options, run_metadata)
    run_metadata_ptr)
srun: error: nid12318: task 670: Exited with exit code 1
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
srun: error: nid12324: task 676: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    raise type(e)(node_def, op, message)
    feed_dict_tensor, options, run_metadata)
tensorflow.python.framework.errors_impl.OutOfRangeErrorsrun: error: nid10586: task 482: Exited with exit code 1
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    options, run_metadata)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

srun: error: nid10709: task 597: Terminated
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeError    run_metadata_ptr)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
srun: error: nid12512: task 864: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    feed_dict_tensor, options, run_metadata)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
srun: error: nid10401: task 309: Terminated
    options, run_metadata)
srun: error: nid10580: task 476: Terminated
  File "CosmoNet.py", line 299, in train
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
srun: error: nid10684: task 576: Terminated
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
tensorflow.python.framework.errors_impl.OutOfRangeError    run_metadata_ptr)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    feed_dict_tensor, options, run_metadata)
srun: error: nid12351: task 703: Exited with exit code 1
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    options, run_metadata)
srun: error: nid12445: task 797: Terminated
  File "CosmoNet.py", line 299, in train
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
tensorflow.python.framework.errors_impl.OutOfRangeError    run_metadata_ptr)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
srun: error: nid10613: task 509: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    feed_dict_tensor, options, run_metadata)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    options, run_metadata)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    raise type(e)(node_def, op, message)
    run_metadata_ptr)
tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    feed_dict_tensor, options, run_metadata)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    run_metadata_ptr)
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
srun: error: nid12424: task 776: Terminated
tensorflow.python.framework.errors_impl.OutOfRangeError    feed_dict_tensor, options, run_metadata)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    options, run_metadata)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    raise type(e)(node_def, op, message)
    run_metadata_ptr)
tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tsrun: error: nid12545: task 897: Terminated
    feed_dict_tensor, options, run_metadata)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    raise type(e)(node_def, op, message)
    run_metadata_ptr)
tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    feed_dict_tensor, options, run_metadata)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    raise type(e)(node_def, op, message)
    run_metadata_ptr)
tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tsrun: error: nid10721: task 609: Terminated
    feed_dict_tensor, options, run_metadata)
srun: error: nid12363: task 715: Exited with exit code 1
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
srun: error: nid10413: task 321: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
tensorflow.python.framework.errors_impl.OutOfRangeError    run_metadata_ptr)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    feed_dict_tensor, options, run_metadata)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    options, run_metadata)
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
srun: error: nid10521: task 421: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    raise type(e)(node_def, op, message)
    feed_dict_tensor, options, run_metadata)
tensorflow.python.framework.errors_impl.OutOfRangeErrorsrun: error: nid09638: task 33: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
srun: error: nid09866: task 248: Exited with exit code 1
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    options, run_metadata)
srun: error: nid09735: task 125: Exited with exit code 1
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeError    run_metadata_ptr)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensosrun: error: nid09839: task 221: Terminated
    feed_dict_tensor, options, run_metadata)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    options, run_metadata)
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    raise type(e)(node_def, op, message)
    feed_dict_tensor, options, run_metadata)
tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    options, run_metadata)
srun: error: nid10434: task 342: Terminated
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
srun: error: nid12645: task 997: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

srun: error: nid09659: task 54: Terminated
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeError    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
srun: error: nid09675: task 70: Exited with exit code 1
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    run_metadata_ptr)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    run_metadata_ptr)
srun: error: nid09876: task 254: Terminated
    options, run_metadata)
srun: error: nid12478: task 830: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    feed_dict_tensor, options, run_metadata)
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
tensorflow.python.framework.errors_impl.OutOfRangeError    options, run_metadata)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    raise type(e)(node_def, op, message)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

tensorflow.python.framework.errors_impl.OutOfRangeError: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    run_metadata_ptr)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    feed_dict_tensor, options, run_metadata)
srun: error: nid10625: task 521: Terminated
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    options, run_metadata)
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
tensorflow.python.framework.errors_impl.OutOfRangeError    raise type(e)(node_def, op, message)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/ttensorflow.python.framework.errors_impl.OutOfRangeErrorensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/trflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

srun: error: nid10542: task 442: Terminated
    run_metadata_ptr)
srun: error: nid12457: task 809: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    feed_dict_tensor, options, run_metadata)
    run_metadata_ptr)
srun: error: nid09671: task 66: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
srun: error: nid09653: task 48: Exited with exit code 1
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    options, run_metadata)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    raise type(e)(node_def, op, message)
    options, run_metadata)
srun: error: nid12557: task 909: Terminated
tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    raise type(e)(node_def, op, message)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensotensorflow.python.framework.errors_impl.OutOfRangeErrorrflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

: srun: error: nid09696: task 87: Terminated
RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
srun: error: nid10650: task 542: Terminated
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/ten    run_metadata_ptr)
sorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorf  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
low/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    options, run_metadata)
srun: error: nid09801: task 187: Terminated
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    raise type(e)(node_def, op, message)
    feed_dict_tensor, options, run_metadata)
tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    options, run_metadata)
srun: error: nid12390: task 742: Terminated
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tsrun: error: nid10748: task 636: Exited with exit code 1
    feed_dict_tensor, options, run_metadata)
srun: error: nid10450: task 354: Terminated
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    run_metadata_ptr)
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
tensorflow.python.framework.errors_impl.OutOfRangeError    feed_dict_tensor, options, run_metadata)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
srun: error: nid12290: task 642: Terminated
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    options, run_metadata)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    raise type(e)(node_def, op, message)
  File "CosmoNet.py", line 299, in train
tensorflow.python.framework.errors_impl.OutOfRangeError    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
srun: error: nid09780: task 166: Terminated
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    run_metadata_ptr)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

srun: error: nid10381: task 293: Exited with exit code 1
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    feed_dict_tensor, options, run_metadata)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    options, run_metadata)
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    raise type(e)(node_def, op, message)
    feed_dict_tensor, options, run_metadata)
tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    options, run_metadata)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
srun: error: nid10662: task 554: Terminated
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
srun: error: nid09708: task 99: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tsrun: error: nid12490: task 842: Terminated
    run_metadata_ptr)
srun: error: nid12590: task 942: Terminated
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

srun: error: nid10483: task 387: Terminated
    feed_dict_tensor, options, run_metadata)
srun: error: nid10375: task 287: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    run_metadata_ptr)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
srun: error: nid09724: task 115: Exited with exit code 1
    feed_dict_tensor, options, run_metadata)
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
tensorflow.python.framework.errors_impl.OutOfRangeError    options, run_metadata)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    raise type(e)(node_def, op, message)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

tensorflow.python.framework.errors_impl.OutOfRangeError: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    run_metadata_ptr)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    feed_dict_tensor, options, run_metadata)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
srun: error: nid12302: task 654: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    options, run_metadata)
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
tensorflow.python.framework.errors_impl.OutOfRangeError    raise type(e)(node_def, op, message)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/ttensorflow.python.framework.errors_impl.OutOfRangeErrorensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/trflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

srun: error: nid12402: task 754: Terminated
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    feed_dict_tensor, options, run_metadata)
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    options, run_metadata)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    raise type(e)(node_def, op, message)
    options, run_metadata)
tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
srun: error: nid09817: task 199: Terminated
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    raise type(e)(node_def, op, message)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensotensorflow.python.framework.errors_impl.OutOfRangeErrorrflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    run_metadata_ptr)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    feed_dict_tensor, options, run_metadata)
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    options, run_metadata)
srun: error: nid12323: task 675: Terminated
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    raise type(e)(node_def, op, message)
    options, run_metadata)
tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    raise type(e)(node_def, op, message)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensotensorflow.python.framework.errors_impl.OutOfRangeErrorsrun: error: nid09702: task 93: Exited with exit code 1
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    run_metadata_ptr)
srun: error: nid12523: task 875: Terminated
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    run_metadata_ptr)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    feed_dict_tensor, options, run_metadata)
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
srun: error: nid10695: task 587: Terminated
tensorflow.python.framework.errors_impl.OutOfRangeErrorsrun: error: nid12602: task 954: Terminated
    options, run_metadata)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    raise type(e)(node_def, op, message)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

tensorflow.python.framework.errors_impl.OutOfRangeError: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

srun: error: nid12623: task 975: Terminated
srun: error: nid09742: task 132: Terminated
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    feed_dict_tensor, options, run_metadata)
srun: error: nid10391: task 299: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    run_metadata_ptr)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    feed_dict_tensor, options, run_metadata)
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
tensorflow.python.framework.errors_impl.OutOfRangeError    options, run_metadata)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    raise type(e)(node_def, op, message)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

tensorflow.python.framework.errors_impl.OutOfRangeError: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    run_metadata_ptr)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensosrun: error: nid10603: task 499: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
srun: error: nid12418: task 770: Exited with exit code 1
    run_metadata_ptr)
srun: error: nid09720: task 111: Terminated
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    feed_dict_tensor, options, run_metadata)
srun: error: nid09611: task 11: Terminated
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
tensorflow.python.framework.errors_impl.OutOfRangeError    options, run_metadata)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
srun: error: nid12335: task 687: Terminated
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    raise type(e)(node_def, op, message)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

tensorflow.python.framework.errors_impl.OutOfRangeError: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
srun: error: nid10711: task 599: Terminated
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    run_metadata_ptr)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    feed_dict_tensor, options, run_metadata)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
srun: error: nid10424: task 332: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    options, run_metadata)
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
tensorflow.python.framework.errors_impl.OutOfRangeError    raise type(e)(node_def, op, message)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/ttensorflow.python.framework.errors_impl.OutOfRangeErrorensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/trflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    run_metadata_ptr)
srun: error: nid09845: task 227: Exited with exit code 1
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    feed_dict_tensor, options, run_metadata)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    options, run_metadata)
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
tensorflow.python.framework.errors_impl.OutOfRangeError    raise type(e)(node_def, op, message)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/ttensorflow.python.framework.errors_impl.OutOfRangeErrorensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/trflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    run_metadata_ptr)
srun: error: nid09796: task 182: Exited with exit code 1
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    run_metadata_ptr)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    feed_dict_tensor, options, run_metadata)
srun: error: nid12347: task 699: Terminated
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    options, run_metadata)
    raise type(e)(node_def, op, message)
srun: error: nid09680: task 71: Exited with exit code 1
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
tensorflow.python.framework.errors_impl.OutOfRangeError    raise type(e)(node_def, op, message)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/ttensorflow.python.framework.errors_impl.OutOfRangeErrorensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
srun: error: nid12306: task 658: Exited with exit code 1
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/trflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    run_metadata_ptr)
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
srun: error: nid10732: task 620: Terminated
    feed_dict_tensor, options, run_metadata)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
srun: error: nid09649: task 44: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
srun: error: nid09758: task 144: Terminated
    options, run_metadata)
srun: error: nid12656: task 1008: Terminated
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    raise type(e)(node_def, op, message)
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeErrorsrun: error: nid12368: task 720: Terminated
tensorflow.python.framework.errors_impl.OutOfRangeError: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensoensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

srun: error: nid12635: task 987: Terminated
    run_metadata_ptr)
srun: error: nid10636: task 532: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    run_metadata_ptr)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    feed_dict_tensor, options, run_metadata)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    options, run_metadata)
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
tensorflow.python.framework.errors_impl.OutOfRangeError    raise type(e)(node_def, op, message)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/ttensorflow.python.framework.errors_impl.OutOfRangeErrorensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/trflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

srun: error: nid12529: task 881: Exited with exit code 1
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
srun: error: nid09655: task 50: Exited with exit code 1
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "CosmoNet.py", line 299, in train
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    run_metadata_ptr)
    run_metadata_ptr)
srun: error: nid12462: task 814: Exited with exit code 1
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    feed_dict_tensor, options, run_metadata)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
srun: error: nid12396: task 748: Exited with exit code 1
    options, run_metadata)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    raise type(e)(node_def, op, message)
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeErrortensorflow.python.framework.errors_impl.OutOfRangeError: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensosrun: error: nid12468: task 820: Terminated
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensosrun: error: nid10436: task 344: Terminated
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    run_metadata_ptr)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
srun: error: nid09891: task 265: Terminated
    feed_dict_tensor, options, run_metadata)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    options, run_metadata)
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
tensorflow.python.framework.errors_impl.OutOfRangeError    raise type(e)(node_def, op, message)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/ttensorflow.python.framework.errors_impl.OutOfRangeErrorensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tsrun: error: nid10532: task 432: Terminated
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

srun: error: nid12568: task 920: Terminated
    run_metadata_ptr)
srun: error: nid09686: task 77: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    run_metadata_ptr)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    feed_dict_tensor, options, run_metadata)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
srun: error: nid10744: task 632: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    options, run_metadata)
srun: error: nid10544: task 444: Terminated
    raise type(e)(node_def, op, message)
srun: error: nid10565: task 465: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
tensorflow.python.framework.errors_impl.OutOfRangeError    raise type(e)(node_def, op, message)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/ttensorflow.python.framework.errors_impl.OutOfRangeErrorensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/trflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    run_metadata_ptr)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    feed_dict_tensor, options, run_metadata)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
srun: error: nid10673: task 565: Terminated
    options, run_metadata)
    raise type(e)(node_def, op, message)
srun: error: nid09833: task 215: Exited with exit code 1
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
tensorflow.python.framework.errors_impl.OutOfRangeError    raise type(e)(node_def, op, message)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
srun: error: nid12480: task 832: Terminated
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tsrun: error: nid12501: task 853: Terminated
tensorflow.python.framework.errors_impl.OutOfRangeErrorensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/trflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    run_metadata_ptr)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
srun: error: nid09791: task 177: Terminated
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    feed_dict_tensor, options, run_metadata)
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    options, run_metadata)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    raise type(e)(node_def, op, message)
srun: error: nid10461: task 365: Terminated
    options, run_metadata)
tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    raise type(e)(node_def, op, message)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensotensorflow.python.framework.errors_impl.OutOfRangeErrorrflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    run_metadata_ptr)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    feed_dict_tensor, options, run_metadata)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    options, run_metadata)
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
tensorflow.python.framework.errors_impl.OutOfRangeError    raise type(e)(node_def, op, message)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/ttensorflow.python.framework.errors_impl.OutOfRangeErrorensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/trflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    run_metadata_ptr)
srun: error: nid12580: task 932: Terminated
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    feed_dict_tensor, options, run_metadata)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    options, run_metadata)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
srun: error: nid09643: task 38: Exited with exit code 1
    raise type(e)(node_def, op, message)
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeErrortensorflow.python.framework.errors_impl.OutOfRangeError: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tsrun: error: nid09803: task 189: Terminated
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensoensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    run_metadata_ptr)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    feed_dict_tensor, options, run_metadata)
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
tensorflow.python.framework.errors_impl.OutOfRangeError    options, run_metadata)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    raise type(e)(node_def, op, message)
srun: error: nid10581: task 477: Terminated
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

srun: error: nid09828: task 210: Terminated
tensorflow.python.framework.errors_impl.OutOfRangeErrorrflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    run_metadata_ptr)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    options, run_metadata)
    run_metadata_ptr)
srun: error: nid10473: task 377: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
srun: error: nid12513: task 865: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    raise type(e)(node_def, op, message)
    feed_dict_tensor, options, run_metadata)
tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tsrun: error: nid09732: task 122: Terminated
    options, run_metadata)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeError  File "CosmoNet.py", line 299, in train
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
slurmstepd: error: *** STEP 11150338.0 ON nid09600 CANCELLED AT 2018-03-23T10:54:52 ***
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    run_metadata_ptr)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    feed_dict_tensor, options, run_metadata)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
srun: error: nid10402: task 310: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    options, run_metadata)
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
tensorflow.python.framework.errors_impl.OutOfRangeError    raise type(e)(node_def, op, message)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/ttensorflow.python.framework.errors_impl.OutOfRangeErrorensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
srun: error: nid10489: task 393: Exited with exit code 1
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/trflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    feed_dict_tensor, options, run_metadata)
    run_metadata_ptr)
srun: error: nid12346: task 698: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    options, run_metadata)
    feed_dict_tensor, options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
srun: error: nid10685: task 577: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    raise type(e)(node_def, op, message)
    options, run_metadata)
tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tsrun: error: nid12325: task 677: Terminated
    raise type(e)(node_def, op, message)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensotensorflow.python.framework.errors_impl.OutOfRangeErrorrflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]
srun: error: nid09626: task 22: Terminated

: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    run_metadata_ptr)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    feed_dict_tensor, options, run_metadata)
srun: error: nid12446: task 798: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    run_metadata_ptr)
    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    feed_dict_tensor, options, run_metadata)
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
tensorflow.python.framework.errors_impl.OutOfRangeError    options, run_metadata)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    raise type(e)(node_def, op, message)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

tensorflow.python.framework.errors_impl.OutOfRangeError: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    run_metadata_ptr)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    feed_dict_tensor, options, run_metadata)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    options, run_metadata)
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
srun: error: nid10506: task 410: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    raise type(e)(node_def, op, message)
    feed_dict_tensor, options, run_metadata)
tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    options, run_metadata)
srun: error: nid09704: task 95: Exited with exit code 1
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
srun: error: nid12425: task 777: Terminated
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeError    run_metadata_ptr)
srun: error: nid09844: task 226: Exited with exit code 1
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    feed_dict_tensor, options, run_metadata)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
srun: error: nid09639: task 34: Terminated
    options, run_metadata)
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
srun: error: nid10614: task 510: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
srun: error: nid12546: task 898: Terminated
    raise type(e)(node_def, op, message)
    feed_dict_tensor, options, run_metadata)
tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    options, run_metadata)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeError    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    run_metadata_ptr)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    feed_dict_tensor, options, run_metadata)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
srun: error: nid12646: task 998: Terminated
    options, run_metadata)
    run_metadata_ptr)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
    raise type(e)(node_def, op, message)
    feed_dict_tensor, options, run_metadata)
srun: error: nid10456: task 360: Exited with exit code 1
tensorflow.python.framework.errors_impl.OutOfRangeErrorsrun: error: nid10414: task 322: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
srun: error: nid10716: task 604: Exited with exit code 1
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    options, run_metadata)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
srun: error: nid12519: task 871: Exited with exit code 1
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
srun: error: nid09660: task 55: Terminated
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
srun: error: nid09748: task 134: Terminated
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tsrun: error: nid12458: task 810: Terminated
    feed_dict_tensor, options, run_metadata)
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensosrun: error: nid12525: task 877: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
srun: error: nid10543: task 443: Terminated
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
srun: error: nid12358: task 710: Terminated
    run_metadata_ptr)
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
tensorflow.python.framework.errors_impl.OutOfRangeError    feed_dict_tensor, options, run_metadata)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tsrun: error: nid10626: task 522: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
srun: error: nid12558: task 910: Terminated
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    options, run_metadata)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
srun: error: nid12419: task 771: Exited with exit code 1
tensorflow.python.framework.errors_impl.OutOfRangeError    run_metadata_ptr)
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    feed_dict_tensor, options, run_metadata)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

srun: error: nid09877: task 255: Terminated
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
    options, run_metadata)
    _,lossTrain,lossL1Train_,train_true_,train_predict_ = sess.run([train_step,loss,lossL1Train,train_true,train_predict])
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    raise type(e)(node_def, op, message)
    run_metadata_ptr)
tensorflow.python.framework.errors_impl.OutOfRangeError  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
srun: error: nid10522: task 422: Terminated
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t    feed_dict_tensor, options, run_metadata)
srun: error: nid09738: task 128: Exited with exit code 1
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

    options, run_metadata)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    run_metadata_ptr)
    raise type(e)(node_def, op, message)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1128, in _run
tensorflow.python.framework.errors_impl.OutOfRangeError    feed_dict_tensor, options, run_metadata)
srun: error: nid10651: task 543: Terminated
: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/t  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1344, in _do_run
ensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tenso    options, run_metadata)
rflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1363, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeError: RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op u'shuffle_batch', defined at:
  File "CosmoNet.py", line 416, in <module>
    NbodySimuDataBatch32, NbodySimuLabelBatch32 = readDataSet(filenames = [hp.Path['train_data']+str(i)+'.tfrecord' for i in range(0,(hyper_parameters_Cosmo.RUNPARAM["num_train"]))])
  File "/global/cscratch1/sd/djbard/cosmoML/github/March23rdRuns/CosmoFlow/1024-test/io_Cosmo.py", line 88, in readDataSet
    allow_smaller_final_batch=True)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 1287, in shuffle_batch
    name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/training/input.py", line 836, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py", line 519, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 2574, in _queue_dequeue_up_to_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3160, in create_op
    op_def=op_def)
  File "/global/common/software/dasrepo/CosmoFlow/March19/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/device:CPU:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

srun: error: nid12370: task 722: Terminated
srun: error: nid12291: task 643: Terminated
srun: error: nid12658: task 1010: Terminated
srun: error: nid10526: task 426: Exited with exit code 1
srun: error: nid10728: task 616: Exited with exit code 1
srun: error: nid09672: task 67: Terminated
srun: error: nid10621: task 517: Exited with exit code 1
srun: error: nid10555: task 455: Terminated
srun: error: nid10734: task 622: Terminated
srun: error: nid12303: task 655: Terminated
srun: error: nid10441: task 349: Exited with exit code 1
srun: error: nid10451: task 355: Terminated
srun: error: nid09893: task 267: Terminated
srun: error: nid10484: task 388: Terminated
srun: error: nid12386: task 738: Exited with exit code 1
srun: error: nid10663: task 555: Terminated
srun: error: nid09703: task 94: Exited with exit code 1
srun: error: nid09781: task 167: Terminated
srun: error: nid12591: task 943: Terminated
srun: error: nid12319: task 671: Exited with exit code 1
srun: error: nid09818: task 200: Terminated
srun: error: nid12403: task 755: Terminated
srun: error: nid10592: task 488: Terminated
srun: error: nid10696: task 588: Terminated
srun: error: nid10392: task 300: Terminated
srun: error: nid12603: task 955: Terminated
srun: error: nid12503: task 855: Terminated
srun: error: nid12524: task 876: Terminated
srun: error: nid09721: task 112: Terminated
srun: error: nid10604: task 500: Terminated
srun: error: nid10496: task 400: Terminated
srun: error: nid12374: task 726: Exited with exit code 1
srun: error: nid12336: task 688: Terminated
srun: error: nid12624: task 976: Terminated
srun: error: nid09633: task 28: Exited with exit code 1
srun: error: nid12436: task 788: Terminated
srun: error: nid09612: task 12: Terminated
srun: error: nid12536: task 888: Terminated
srun: error: nid09600: task 0: Terminated
srun: error: nid10669: task 561: Exited with exit code 1
srun: error: nid12309: task 661: Exited with exit code 1
srun: error: nid10667: task 559: Exited with exit code 1
srun: error: nid10404: task 312: Terminated
srun: error: nid09851: task 233: Terminated
srun: error: nid10712: task 600: Terminated
srun: error: nid10733: task 621: Terminated
srun: error: nid09650: task 45: Terminated
srun: error: nid12348: task 700: Terminated
srun: error: nid10533: task 433: Terminated
srun: error: nid09759: task 145: Terminated
srun: error: nid10425: task 333: Terminated
srun: error: nid12636: task 988: Terminated
srun: error: nid09863: task 245: Terminated
srun: error: nid12469: task 821: Terminated
srun: error: nid12448: task 800: Terminated
srun: error: nid12585: task 937: Exited with exit code 1
srun: error: nid12548: task 900: Terminated
srun: error: nid10739: task 627: Exited with exit code 1
srun: error: nid12569: task 921: Terminated
srun: error: nid09662: task 57: Terminated
srun: error: nid09687: task 78: Terminated
srun: error: nid10637: task 533: Terminated
srun: error: nid10528: task 428: Exited with exit code 1
srun: error: nid10566: task 466: Terminated
srun: error: nid09792: task 178: Terminated
srun: error: nid10653: task 545: Terminated
srun: error: nid10545: task 445: Terminated
srun: error: nid12314: task 666: Terminated
srun: error: nid12508: task 860: Exited with exit code 1
srun: error: nid09656: task 51: Exited with exit code 1
srun: error: nid10474: task 378: Terminated
srun: error: nid12581: task 933: Terminated
srun: error: nid09699: task 90: Terminated
srun: error: nid12293: task 645: Terminated
srun: error: nid12393: task 745: Terminated
srun: error: nid10582: task 478: Terminated
srun: error: nid10674: task 566: Terminated
srun: error: nid10378: task 290: Terminated
srun: error: nid09808: task 190: Terminated
srun: error: nid12514: task 866: Terminated
srun: error: nid10480: task 384: Exited with exit code 1
srun: error: nid10486: task 390: Terminated
srun: error: nid10631: task 527: Exited with exit code 1
srun: error: nid12414: task 766: Terminated
srun: error: nid10686: task 578: Terminated
srun: error: nid12326: task 678: Terminated
srun: error: nid10718: task 606: Exited with exit code 1
srun: error: nid12614: task 966: Terminated
srun: error: nid09705: task 96: Exited with exit code 1
srun: error: nid10398: task 306: Exited with exit code 1
srun: error: nid12426: task 778: Terminated
srun: error: nid10403: task 311: Terminated
srun: error: nid09798: task 184: Exited with exit code 1
srun: error: nid12587: task 939: Exited with exit code 1
srun: error: nid10706: task 594: Exited with exit code 1
srun: error: nid12447: task 799: Terminated
srun: error: nid10723: task 611: Terminated
srun: error: nid09887: task 261: Exited with exit code 1
srun: error: nid10442: task 350: Exited with exit code 1
srun: error: nid10415: task 323: Terminated
srun: error: nid09749: task 135: Terminated
srun: error: nid12547: task 899: Terminated
srun: error: nid09661: task 56: Terminated
srun: error: nid10550: task 450: Exited with exit code 1
srun: error: nid12664: task 1016: Exited with exit code 1
srun: error: nid12359: task 711: Terminated
srun: error: nid10627: task 523: Terminated
srun: error: nid12459: task 811: Terminated
srun: error: nid10523: task 423: Terminated
srun: error: nid10427: task 335: Terminated
srun: error: nid10735: task 623: Terminated
srun: error: nid12365: task 717: Exited with exit code 1
srun: error: nid12559: task 911: Terminated
srun: error: nid10664: task 556: Terminated
srun: error: nid12471: task 823: Terminated
srun: error: nid12371: task 723: Terminated
srun: error: nid12659: task 1011: Terminated
srun: error: nid10452: task 356: Terminated
srun: error: nid10556: task 456: Terminated
srun: error: nid09689: task 80: Terminated
srun: error: nid12392: task 744: Terminated
srun: error: nid10633: task 529: Exited with exit code 1
srun: error: nid09601: task 1: Terminated
srun: error: nid10588: task 484: Exited with exit code 1
srun: error: nid12492: task 844: Terminated
srun: error: nid09894: task 268: Terminated
srun: error: nid12304: task 656: Terminated
srun: error: nid12592: task 944: Terminated
srun: error: nid09710: task 101: Terminated
srun: error: nid12408: task 760: Exited with exit code 1
srun: error: nid12504: task 856: Terminated
srun: error: nid09819: task 201: Terminated
srun: error: nid09683: task 74: Exited with exit code 1
srun: error: nid10593: task 489: Terminated
srun: error: nid09729: task 119: Exited with exit code 1
srun: error: nid09634: task 29: Exited with exit code 1
srun: error: nid12310: task 662: Exited with exit code 1
srun: error: nid10393: task 301: Terminated
srun: error: nid12604: task 956: Terminated
srun: error: nid12625: task 977: Terminated
srun: error: nid10697: task 589: Terminated
srun: error: nid12610: task 962: Exited with exit code 1
srun: error: nid12663: task 1015: Exited with exit code 1
srun: error: nid12337: task 689: Terminated
srun: error: nid12437: task 789: Terminated
srun: error: nid12498: task 850: Exited with exit code 1
srun: error: nid09788: task 174: Exited with exit code 1
srun: error: nid09613: task 13: Terminated
srun: error: nid12630: task 982: Exited with exit code 1
srun: error: nid09727: task 117: Exited with exit code 1
srun: error: nid12537: task 889: Terminated
srun: error: nid10421: task 329: Exited with exit code 1
srun: error: nid10713: task 601: Terminated
srun: error: nid09852: task 234: Terminated
srun: error: nid09739: task 129: Exited with exit code 1
srun: error: nid10426: task 334: Terminated
srun: error: nid09831: task 213: Terminated
srun: error: nid09651: task 46: Terminated
srun: error: nid12563: task 915: Exited with exit code 1
srun: error: nid12637: task 989: Terminated
srun: error: nid10605: task 501: Terminated
srun: error: nid10405: task 313: Terminated
srun: error: nid09837: task 219: Exited with exit code 1
srun: error: nid12375: task 727: Exited with exit code 1
srun: error: nid10707: task 595: Exited with exit code 1
srun: error: nid10638: task 534: Terminated
srun: error: nid10534: task 434: Terminated
srun: error: nid10746: task 634: Terminated
srun: error: nid10740: task 628: Exited with exit code 1
srun: error: nid09688: task 79: Terminated
srun: error: nid12449: task 801: Terminated
srun: error: nid12570: task 922: Terminated
srun: error: nid12575: task 927: Exited with exit code 1
srun: error: nid12382: task 734: Terminated
srun: error: nid10438: task 346: Terminated
srun: error: nid10509: task 413: Terminated
srun: error: nid12510: task 862: Exited with exit code 1
srun: error: nid12549: task 901: Terminated
srun: error: nid12649: task 1001: Terminated
srun: error: nid09772: task 158: Terminated
srun: error: nid09793: task 179: Terminated
srun: error: nid12294: task 646: Terminated
srun: error: nid10372: task 284: Exited with exit code 1
srun: error: nid09700: task 91: Terminated
srun: error: nid12488: task 840: Exited with exit code 1
srun: error: nid10458: task 362: Exited with exit code 1
srun: error: nid12598: task 950: Exited with exit code 1
srun: error: nid12582: task 934: Terminated
srun: error: nid12482: task 834: Terminated
srun: error: nid09905: task 279: Terminated
srun: error: nid12315: task 667: Terminated
srun: error: nid10654: task 546: Terminated
srun: error: nid12415: task 767: Terminated
srun: error: nid10454: task 358: Terminated
srun: error: nid10517: task 417: Exited with exit code 1
srun: error: nid12515: task 867: Terminated
srun: error: nid10379: task 291: Terminated
srun: error: nid10475: task 379: Terminated
srun: error: nid12394: task 746: Terminated
srun: error: nid09712: task 103: Terminated
srun: error: nid09669: task 64: Exited with exit code 1
srun: error: nid12586: task 938: Exited with exit code 1
srun: error: nid12327: task 679: Terminated
srun: error: nid09842: task 224: Terminated
srun: error: nid10373: task 285: Exited with exit code 1
srun: error: nid10508: task 412: Terminated
srun: error: nid09641: task 36: Terminated
srun: error: nid10699: task 591: Terminated
srun: error: nid09776: task 162: Exited with exit code 1
srun: error: nid10595: task 491: Terminated
srun: error: nid10524: task 424: Terminated
srun: error: nid10416: task 324: Terminated
srun: error: nid12339: task 691: Terminated
srun: error: nid12460: task 812: Terminated
srun: error: nid12360: task 712: Terminated
srun: error: nid09858: task 240: Exited with exit code 1
srun: error: nid10634: task 530: Exited with exit code 1
srun: error: nid09607: task 7: Exited with exit code 1
srun: error: nid10589: task 485: Exited with exit code 1
srun: error: nid10453: task 357: Terminated
srun: error: nid12372: task 724: Terminated
srun: error: nid12433: task 785: Exited with exit code 1
srun: error: nid09674: task 69: Terminated
srun: error: nid10611: task 507: Exited with exit code 1
srun: error: nid12660: task 1012: Terminated
srun: error: nid10736: task 624: Terminated
srun: error: nid10557: task 457: Terminated
srun: error: nid12560: task 912: Terminated
srun: error: nid10628: task 524: Terminated
srun: error: nid10536: task 436: Terminated
srun: error: nid12455: task 807: Exited with exit code 1
srun: error: nid12466: task 818: Exited with exit code 1
srun: error: nid09783: task 169: Terminated
srun: error: nid12493: task 845: Terminated
srun: error: nid12305: task 657: Terminated
srun: error: nid12454: task 806: Exited with exit code 1
srun: error: nid09711: task 102: Terminated
srun: error: nid12572: task 924: Terminated
srun: error: nid09777: task 163: Exited with exit code 1
srun: error: nid09795: task 181: Terminated
srun: error: nid10569: task 469: Terminated
srun: error: nid09820: task 202: Terminated
srun: error: nid09901: task 275: Exited with exit code 1
srun: error: nid10388: task 296: Exited with exit code 1
srun: error: nid12505: task 857: Terminated
srun: error: nid10681: task 573: Exited with exit code 1
srun: error: nid10465: task 369: Terminated
srun: error: nid10498: task 402: Terminated
srun: error: nid10677: task 569: Terminated
srun: error: nid10683: task 575: Exited with exit code 1
srun: error: nid12605: task 957: Terminated
srun: error: nid12317: task 669: Terminated
srun: error: nid12444: task 796: Exited with exit code 1
srun: error: nid12417: task 769: Terminated
srun: error: nid10394: task 302: Terminated
srun: error: nid09723: task 114: Terminated
srun: error: nid10698: task 590: Terminated
srun: error: nid12576: task 928: Exited with exit code 1
srun: error: nid10714: task 602: Terminated
srun: error: nid12421: task 773: Exited with exit code 1
srun: error: nid10606: task 502: Terminated
srun: error: nid12538: task 890: Terminated
srun: error: nid12350: task 702: Terminated
srun: error: nid12344: task 696: Exited with exit code 1
srun: error: nid09631: task 26: Terminated
srun: error: nid12517: task 869: Terminated
srun: error: nid10510: task 414: Terminated
srun: error: nid10535: task 435: Terminated
srun: error: nid10618: task 514: Terminated
srun: error: nid12450: task 802: Terminated
srun: error: nid09736: task 126: Terminated
srun: error: nid12638: task 990: Terminated
srun: error: nid10406: task 314: Terminated
srun: error: nid09865: task 247: Terminated
srun: error: nid10547: task 447: Terminated
srun: error: nid09664: task 59: Terminated
srun: error: nid10639: task 535: Terminated
srun: error: nid12362: task 714: Terminated
srun: error: nid10439: task 347: Terminated
srun: error: nid12550: task 902: Terminated
srun: error: nid12383: task 735: Terminated
srun: error: nid12671: task 1023: Terminated
srun: error: nid10747: task 635: Terminated
srun: error: nid09773: task 159: Terminated
srun: error: nid09885: task 259: Terminated
srun: error: nid12650: task 1002: Terminated
srun: error: nid10368: task 280: Terminated
srun: error: nid12295: task 647: Terminated
srun: error: nid12483: task 835: Terminated
srun: error: nid10476: task 380: Terminated
srun: error: nid10559: task 459: Terminated
srun: error: nid09701: task 92: Terminated
srun: error: nid10455: task 359: Terminated
srun: error: nid09810: task 192: Terminated
srun: error: nid10600: task 496: Exited with exit code 1
srun: error: nid12416: task 768: Terminated
srun: error: nid12495: task 847: Terminated
srun: error: nid12328: task 680: Terminated
srun: error: nid12595: task 947: Terminated
srun: error: nid09713: task 104: Terminated
srun: error: nid10584: task 480: Terminated
srun: error: nid12516: task 868: Terminated
srun: error: nid09604: task 4: Terminated
srun: error: nid09767: task 153: Exited with exit code 1
srun: error: nid12521: task 873: Exited with exit code 1
srun: error: nid10688: task 580: Terminated
srun: error: nid12456: task 808: Exited with exit code 1
srun: error: nid12616: task 968: Terminated
srun: error: nid10596: task 492: Terminated
srun: error: nid10488: task 392: Terminated
srun: error: nid12428: task 780: Terminated
srun: error: nid10396: task 304: Terminated
srun: error: nid09843: task 225: Terminated
srun: error: nid12361: task 713: Terminated
srun: error: nid12544: task 896: Exited with exit code 1
srun: error: nid09751: task 137: Terminated
srun: error: nid12528: task 880: Terminated
srun: error: nid09642: task 37: Terminated
srun: error: nid10725: task 613: Terminated
srun: error: nid10541: task 441: Exited with exit code 1
srun: error: nid12311: task 663: Exited with exit code 1
srun: error: nid10525: task 425: Terminated
srun: error: nid12340: task 692: Terminated
srun: error: nid09855: task 237: Terminated
srun: error: nid09789: task 175: Exited with exit code 1
srun: error: nid12440: task 792: Terminated
srun: error: nid10704: task 592: Terminated
srun: error: nid12389: task 741: Exited with exit code 1
srun: error: nid12561: task 913: Terminated
srun: error: nid12540: task 892: Terminated
srun: error: nid12377: task 729: Exited with exit code 1
srun: error: nid10429: task 337: Terminated
srun: error: nid12461: task 813: Terminated
srun: error: nid10537: task 437: Terminated
srun: error: nid10649: task 541: Exited with exit code 1
srun: error: nid09654: task 49: Terminated
srun: error: nid12373: task 725: Terminated
srun: error: nid09763: task 149: Terminated
srun: error: nid12473: task 825: Terminated
srun: error: nid10558: task 458: Terminated
srun: error: nid12661: task 1013: Terminated
srun: error: nid09784: task 170: Terminated
srun: error: nid10645: task 537: Terminated
srun: error: nid12573: task 925: Terminated
srun: error: nid09896: task 270: Terminated
srun: error: nid09691: task 82: Terminated
srun: error: nid10466: task 370: Terminated
srun: error: nid10666: task 558: Terminated
srun: error: nid12594: task 946: Terminated
srun: error: nid10570: task 470: Terminated
srun: error: nid10749: task 637: Terminated
srun: error: nid09821: task 203: Terminated
srun: error: nid12406: task 758: Terminated
srun: error: nid10395: task 303: Terminated
srun: error: nid12506: task 858: Terminated
srun: error: nid09615: task 15: Terminated
srun: error: nid10678: task 570: Terminated
srun: error: nid12606: task 958: Terminated
srun: error: nid10499: task 403: Terminated
srun: error: nid10478: task 382: Terminated
srun: error: nid10607: task 503: Terminated
srun: error: nid10370: task 282: Terminated
srun: error: nid09632: task 27: Terminated
srun: error: nid12518: task 870: Terminated
srun: error: nid12439: task 791: Terminated
srun: error: nid12539: task 891: Terminated
srun: error: nid09737: task 127: Terminated
srun: error: nid12639: task 991: Terminated
srun: error: nid12618: task 970: Terminated
srun: error: nid10520: task 420: Exited with exit code 1
srun: error: nid10511: task 415: Terminated
srun: error: nid10407: task 315: Terminated
srun: error: nid12451: task 803: Terminated
srun: error: nid10715: task 603: Terminated
srun: error: nid09762: task 148: Terminated
srun: error: nid09665: task 60: Terminated
srun: error: nid09779: task 165: Exited with exit code 1
srun: error: nid10619: task 515: Terminated
srun: error: nid10419: task 327: Terminated
srun: error: nid09670: task 65: Exited with exit code 1
srun: error: nid12551: task 903: Terminated
srun: error: nid10440: task 348: Terminated
srun: error: nid10727: task 615: Terminated
srun: error: nid12384: task 736: Terminated
srun: error: nid09774: task 160: Terminated
srun: error: nid12651: task 1003: Terminated
srun: error: nid10656: task 548: Terminated
srun: error: nid09681: task 72: Terminated
srun: error: nid12463: task 815: Terminated
srun: error: nid10548: task 448: Terminated
srun: error: nid10477: task 381: Terminated
srun: error: nid09886: task 260: Terminated
srun: error: nid12584: task 936: Terminated
srun: error: nid12484: task 836: Terminated
srun: error: nid10560: task 460: Terminated
srun: error: nid09811: task 193: Terminated
srun: error: nid10668: task 560: Terminated
srun: error: nid12308: task 660: Terminated
srun: error: nid12496: task 848: Terminated
srun: error: nid12296: task 648: Terminated
srun: error: nid12596: task 948: Terminated
srun: error: nid10689: task 581: Terminated
srun: error: nid10585: task 481: Terminated
srun: error: nid09605: task 5: Terminated
srun: error: nid09714: task 105: Terminated
srun: error: nid12617: task 969: Terminated
srun: error: nid09823: task 205: Terminated
srun: error: nid12329: task 681: Terminated
srun: error: nid12429: task 781: Terminated
srun: error: nid10597: task 493: Terminated
srun: error: nid10397: task 305: Terminated
srun: error: nid12341: task 693: Terminated
srun: error: nid10705: task 593: Terminated
srun: error: nid12629: task 981: Terminated
srun: error: nid10418: task 326: Terminated
srun: error: nid10501: task 405: Terminated
srun: error: nid09856: task 238: Terminated
srun: error: nid09752: task 138: Terminated
srun: error: nid12541: task 893: Terminated
srun: error: nid12441: task 793: Terminated
srun: error: nid10630: task 526: Terminated
srun: error: nid12641: task 993: Terminated
srun: error: nid10609: task 505: Terminated
srun: error: nid10538: task 438: Terminated
srun: error: nid09764: task 150: Terminated
srun: error: nid10738: task 626: Terminated
srun: error: nid12562: task 914: Terminated
srun: error: nid12662: task 1014: Terminated
srun: error: nid09785: task 171: Terminated
srun: error: nid10750: task 638: Terminated
srun: error: nid09868: task 250: Terminated
srun: error: nid12474: task 826: Terminated
srun: error: nid09692: task 83: Terminated
srun: error: nid10430: task 338: Terminated
srun: error: nid09897: task 271: Terminated
srun: error: nid12574: task 926: Terminated
srun: error: nid10646: task 538: Terminated
srun: error: nid12407: task 759: Terminated
srun: error: nid10467: task 371: Terminated
srun: error: nid12307: task 659: Terminated
srun: error: nid12507: task 859: Terminated
srun: error: nid09797: task 183: Terminated
srun: error: nid12486: task 838: Terminated
srun: error: nid09725: task 116: Terminated
srun: error: nid10479: task 383: Terminated
srun: error: nid10571: task 471: Terminated
srun: error: nid10371: task 283: Terminated
srun: error: nid12607: task 959: Terminated
srun: error: nid10679: task 571: Terminated
srun: error: nid09620: task 16: Terminated
srun: error: nid10608: task 504: Terminated
srun: error: nid12352: task 704: Terminated
srun: error: nid10500: task 404: Terminated
srun: error: nid09834: task 216: Terminated
srun: error: nid10691: task 583: Terminated
srun: error: nid10587: task 483: Terminated
srun: error: nid12619: task 971: Terminated
srun: error: nid10516: task 416: Terminated
srun: error: nid10408: task 316: Terminated
srun: error: nid12640: task 992: Terminated
srun: error: nid10620: task 516: Terminated
srun: error: nid09867: task 249: Terminated
srun: error: nid09846: task 228: Terminated
srun: error: nid12452: task 804: Terminated
srun: error: nid12364: task 716: Terminated
srun: error: nid09666: task 61: Terminated
srun: error: nid12652: task 1004: Terminated
srun: error: nid10420: task 328: Terminated
srun: error: nid12385: task 737: Terminated
srun: error: nid12552: task 904: Terminated
srun: error: nid10549: task 449: Terminated
srun: error: nid12464: task 816: Terminated
srun: error: nid10657: task 549: Terminated
srun: error: nid12485: task 837: Terminated
srun: error: nid09775: task 161: Terminated
srun: error: nid12564: task 916: Terminated
srun: error: nid09682: task 73: Terminated
srun: error: nid12397: task 749: Terminated
srun: error: nid09812: task 194: Terminated
srun: error: nid10457: task 361: Terminated
srun: error: nid12297: task 649: Terminated
srun: error: nid09787: task 173: Terminated
srun: error: nid10561: task 461: Terminated
srun: error: nid09899: task 273: Terminated
srun: error: nid10690: task 582: Terminated
srun: error: nid10382: task 294: Terminated
srun: error: nid12597: task 949: Terminated
srun: error: nid09606: task 6: Terminated
srun: error: nid12497: task 849: Terminated
srun: error: nid09715: task 106: Terminated
srun: error: nid09824: task 206: Terminated
srun: error: nid12409: task 761: Terminated
srun: error: nid12330: task 682: Terminated
srun: error: nid12430: task 782: Terminated
srun: error: nid10598: task 494: Terminated
srun: error: nid10490: task 394: Terminated
srun: error: nid09753: task 139: Terminated
srun: error: nid09728: task 118: Terminated
srun: error: nid09644: task 39: Terminated
srun: error: nid12530: task 882: Terminated
srun: error: nid09622: task 18: Terminated
srun: error: nid12342: task 694: Terminated
srun: error: nid12442: task 794: Terminated
srun: error: nid09857: task 239: Terminated
srun: error: nid10527: task 427: Terminated
srun: error: nid10502: task 406: Terminated
srun: error: nid12542: task 894: Terminated
srun: error: nid10610: task 506: Terminated
srun: error: nid10431: task 339: Terminated
srun: error: nid09898: task 272: Terminated
srun: error: nid09765: task 151: Terminated
srun: error: nid12642: task 994: Terminated
srun: error: nid09869: task 251: Terminated
srun: error: nid10647: task 539: Terminated
srun: error: nid12475: task 827: Terminated
srun: error: nid09786: task 172: Terminated
srun: error: nid10539: task 439: Terminated
srun: error: nid10551: task 451: Terminated
srun: error: nid10468: task 372: Terminated
srun: error: nid10751: task 639: Terminated
srun: error: nid09693: task 84: Terminated
srun: error: nid10443: task 351: Terminated
srun: error: nid10576: task 472: Terminated
srun: error: nid12387: task 739: Terminated
srun: error: nid12608: task 960: Terminated
srun: error: nid12487: task 839: Terminated
srun: error: nid12320: task 672: Terminated
srun: error: nid12420: task 772: Terminated
srun: error: nid10680: task 572: Terminated
srun: error: nid09621: task 17: Terminated
srun: error: nid09814: task 196: Terminated
srun: error: nid12520: task 872: Terminated
srun: error: nid09835: task 217: Terminated
srun: error: nid10692: task 584: Terminated
srun: error: nid12332: task 684: Terminated
srun: error: nid12353: task 705: Terminated
srun: error: nid12620: task 972: Terminated
srun: error: nid12432: task 784: Terminated
srun: error: nid09646: task 41: Terminated
srun: error: nid10717: task 605: Terminated
srun: error: nid10729: task 617: Terminated
srun: error: nid12453: task 805: Terminated
srun: error: nid12653: task 1005: Terminated
srun: error: nid12553: task 905: Terminated
srun: error: nid10409: task 317: Terminated
srun: error: nid09847: task 229: Terminated
srun: error: nid09667: task 62: Terminated
srun: error: nid10529: task 429: Terminated
srun: error: nid10741: task 629: Terminated
srun: error: nid10658: task 550: Terminated
srun: error: nid12565: task 917: Terminated
srun: error: nid12465: task 817: Terminated
srun: error: nid12298: task 650: Terminated
srun: error: nid09888: task 262: Terminated
srun: error: nid12665: task 1017: Terminated
srun: error: nid12398: task 750: Terminated
srun: error: nid10562: task 462: Terminated
srun: error: nid09813: task 195: Terminated
srun: error: nid10383: task 295: Terminated
srun: error: nid09900: task 274: Terminated
srun: error: nid12410: task 762: Terminated
srun: error: nid10670: task 562: Terminated
srun: error: nid10491: task 395: Terminated
srun: error: nid09825: task 207: Terminated
srun: error: nid09716: task 107: Terminated
srun: error: nid10470: task 374: Terminated
srun: error: nid12431: task 783: Terminated
srun: error: nid12331: task 683: Terminated
srun: error: nid10599: task 495: Terminated
srun: error: nid10399: task 307: Terminated
srun: error: nid12343: task 695: Terminated
srun: error: nid10503: task 407: Terminated
srun: error: nid09645: task 40: Terminated
srun: error: nid09754: task 140: Terminated
srun: error: nid12531: task 883: Terminated
srun: error: nid12443: task 795: Terminated
srun: error: nid12631: task 983: Terminated
srun: error: nid09623: task 19: Terminated
srun: error: nid09657: task 52: Terminated
srun: error: nid10719: task 607: Terminated
srun: error: nid12543: task 895: Terminated
srun: error: nid09766: task 152: Terminated
srun: error: nid10632: task 528: Terminated
srun: error: nid10648: task 540: Terminated
srun: error: nid10540: task 440: Terminated
srun: error: nid12355: task 707: Terminated
srun: error: nid10432: task 340: Terminated
srun: error: nid12376: task 728: Terminated
srun: error: nid12643: task 995: Terminated
srun: error: nid09870: task 252: Terminated
srun: error: nid12476: task 828: Terminated
srun: error: nid10660: task 552: Terminated
srun: error: nid12288: task 640: Terminated
srun: error: nid10448: task 352: Terminated
srun: error: nid10552: task 452: Terminated
srun: error: nid12388: task 740: Terminated
srun: error: nid10469: task 373: Terminated
srun: error: nid09799: task 185: Terminated
srun: error: nid10577: task 473: Terminated
srun: error: nid12509: task 861: Terminated
srun: error: nid12321: task 673: Terminated
srun: error: nid12588: task 940: Terminated
srun: error: nid09694: task 85: Terminated
srun: error: nid12609: task 961: Terminated
srun: error: nid10481: task 385: Terminated
srun: error: nid09706: task 97: Terminated
srun: error: nid09815: task 197: Terminated
srun: error: nid10693: task 585: Terminated
srun: error: nid09836: task 218: Terminated
srun: error: nid12621: task 973: Terminated
srun: error: nid09635: task 30: Terminated
srun: error: nid10493: task 397: Terminated
srun: error: nid09740: task 130: Terminated
srun: error: nid10410: task 318: Terminated
srun: error: nid12333: task 685: Terminated
srun: error: nid12354: task 706: Terminated
srun: error: nid10389: task 297: Terminated
srun: error: nid10518: task 418: Terminated
srun: error: nid10422: task 330: Terminated
srun: error: nid10622: task 518: Terminated
srun: error: nid12533: task 885: Terminated
srun: error: nid12654: task 1006: Terminated
srun: error: nid09647: task 42: Terminated
srun: error: nid12633: task 985: Terminated
srun: error: nid12554: task 906: Terminated
srun: error: nid10730: task 618: Terminated
srun: error: nid10530: task 430: Terminated
srun: error: nid12366: task 718: Terminated
srun: error: nid09756: task 142: Terminated
srun: error: nid09668: task 63: Terminated
srun: error: nid09860: task 242: Terminated
srun: error: nid09848: task 230: Terminated
srun: error: nid10659: task 551: Terminated
srun: error: nid09684: task 75: Terminated
srun: error: nid10459: task 363: Terminated
srun: error: nid12378: task 730: Terminated
srun: error: nid12666: task 1018: Terminated
srun: error: nid10742: task 630: Terminated
srun: error: nid12566: task 918: Terminated
srun: error: nid09889: task 263: Terminated
srun: error: nid12299: task 651: Terminated
srun: error: nid12399: task 751: Terminated
srun: error: nid10563: task 463: Terminated
srun: error: nid10671: task 563: Terminated
srun: error: nid09608: task 8: Terminated
srun: error: nid12499: task 851: Terminated
srun: error: nid10471: task 375: Terminated
srun: error: nid12599: task 951: Terminated
srun: error: nid12411: task 763: Terminated
srun: error: nid10579: task 475: Terminated
srun: error: nid09826: task 208: Terminated
srun: error: nid09717: task 108: Terminated
srun: error: nid12511: task 863: Terminated
srun: error: nid09624: task 20: Terminated
srun: error: nid12532: task 884: Terminated
srun: error: nid12632: task 984: Terminated
srun: error: nid09730: task 120: Terminated
srun: error: nid10492: task 396: Terminated
srun: error: nid12611: task 963: Terminated
srun: error: nid10504: task 408: Terminated
srun: error: nid10400: task 308: Terminated
srun: error: nid10612: task 508: Terminated
srun: error: nid09859: task 241: Terminated
srun: error: nid09658: task 53: Terminated
srun: error: nid09755: task 141: Terminated
srun: error: nid10708: task 596: Terminated
srun: error: nid10720: task 608: Terminated
srun: error: nid10433: task 341: Terminated
srun: error: nid10412: task 320: Terminated
srun: error: nid12356: task 708: Terminated
srun: error: nid09838: task 220: Terminated
srun: error: nid12644: task 996: Terminated
srun: error: nid12477: task 829: Terminated
srun: error: nid10553: task 453: Terminated
srun: error: nid12577: task 929: Terminated
srun: error: nid12289: task 641: Terminated
srun: error: nid09871: task 253: Terminated
srun: error: nid10449: task 353: Terminated
srun: error: nid09695: task 86: Terminated
srun: error: nid12556: task 908: Terminated
srun: Force Terminated job step 11150338.0
